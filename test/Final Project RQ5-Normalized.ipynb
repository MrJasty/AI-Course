{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out only team USA and predict the current points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#general\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "#pandas\n",
    "import pandas as pd\n",
    "\n",
    "#TensorFlow\n",
    "import tensorflow as tf\n",
    "#numpy\n",
    "import numpy as np\n",
    "\n",
    "#matplotlib.pylot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.core import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting File:\n",
      "    rank country_full country_abrv  total_points  previous_points  \\\n",
      "20    28          USA          USA        779.25              733   \n",
      "21    29          USA          USA        779.25              779   \n",
      "22    28          USA          USA        798.28              779   \n",
      "23    22          USA          USA        865.10              798   \n",
      "24    19          USA          USA        959.80              865   \n",
      "25    13          USA          USA        995.75              960   \n",
      "26    13          USA          USA       1039.80              996   \n",
      "27    14          USA          USA       1018.71             1040   \n",
      "28    14          USA          USA       1018.71             1019   \n",
      "29    14          USA          USA       1018.71             1019   \n",
      "30    13          USA          USA       1043.70             1019   \n",
      "31    14          USA          USA       1017.32             1044   \n",
      "32    13          USA          USA       1014.50             1017   \n",
      "33    14          USA          USA       1014.50             1015   \n",
      "34    13          USA          USA       1034.82             1015   \n",
      "35    15          USA          USA        988.66             1035   \n",
      "36    18          USA          USA        936.94              989   \n",
      "37    17          USA          USA        936.09              937   \n",
      "38    23          USA          USA        861.95              936   \n",
      "39    28          USA          USA        835.79              862   \n",
      "40    27          USA          USA        835.79              836   \n",
      "41    27          USA          USA        835.79              836   \n",
      "42    31          USA          USA        824.25              836   \n",
      "43    32          USA          USA        827.97              824   \n",
      "44    27          USA          USA        815.26              828   \n",
      "45    28          USA          USA        824.50              815   \n",
      "46    27          USA          USA        823.24              825   \n",
      "47    34          USA          USA        748.25              823   \n",
      "48    29          USA          USA        815.57              748   \n",
      "49    28          USA          USA        822.57              816   \n",
      "..   ...          ...          ...           ...              ...   \n",
      "53    32          USA          USA        784.45              784   \n",
      "54    32          USA          USA        792.84              784   \n",
      "55    30          USA          USA        791.89              793   \n",
      "56    29          USA          USA        809.62              792   \n",
      "57    29          USA          USA        810.09              810   \n",
      "58    31          USA          USA        802.75              810   \n",
      "59    25          USA          USA        855.43              803   \n",
      "60    26          USA          USA        848.13              855   \n",
      "61    22          USA          USA        874.03              848   \n",
      "62    24          USA          USA        852.35              874   \n",
      "63    28          USA          USA        822.00              852   \n",
      "64    28          USA          USA        822.00              822   \n",
      "65    28          USA          USA        822.00              822   \n",
      "66    29          USA          USA        818.10              822   \n",
      "67    30          USA          USA        818.17              818   \n",
      "68    23          USA          USA        846.21              818   \n",
      "69    23          USA          USA        846.91              846   \n",
      "70    23          USA          USA        861.29              847   \n",
      "71    35          USA          USA        751.69              861   \n",
      "72    26          USA          USA        865.43              752   \n",
      "73    28          USA          USA        827.87              865   \n",
      "74    27          USA          USA        842.51              828   \n",
      "75    24          USA          USA        867.03              843   \n",
      "76    24          USA          USA        867.03              867   \n",
      "77    25          USA          USA        867.03              867   \n",
      "78    24          USA          USA        871.42              867   \n",
      "79    25          USA          USA        875.50              871   \n",
      "80    24          USA          USA        879.64              876   \n",
      "81    24          USA          USA        880.25              880   \n",
      "82    25          USA          USA        872.68              880   \n",
      "\n",
      "    rank_change  cur_year_avg  cur_year_avg_weighted  last_year_avg  \\\n",
      "20          5.0        393.69                 393.69         350.25   \n",
      "21          NaN        393.69                 393.69         350.25   \n",
      "22          1.0        423.22                 423.22         341.26   \n",
      "23          6.0        536.98                 536.98         256.45   \n",
      "24          3.0        648.60                 648.60         256.45   \n",
      "25          6.0        662.73                 662.73         339.52   \n",
      "26          0.0        684.93                 684.93         407.37   \n",
      "27          NaN        654.09                 654.09         414.44   \n",
      "28          0.0        654.09                 654.09         414.44   \n",
      "29          0.0        654.09                 654.09         414.44   \n",
      "30          1.0        695.82                 695.82         362.11   \n",
      "31          NaN        665.57                 665.57         348.32   \n",
      "32          1.0        640.42                 640.42         393.69   \n",
      "33          NaN        640.42                 640.42         393.69   \n",
      "34          1.0        650.42                 650.42         423.22   \n",
      "35         -2.0        554.94                 554.94         584.27   \n",
      "36         -3.0        466.52                 466.52         648.60   \n",
      "37          1.0        444.73                 444.73         662.73   \n",
      "38         -6.0        337.65                 337.65         684.93   \n",
      "39         -5.0        327.31                 327.31         654.09   \n",
      "40          1.0        327.31                 327.31         654.09   \n",
      "41          0.0        327.31                 327.31         654.09   \n",
      "42         -4.0        305.98                 305.98         695.82   \n",
      "43          NaN        326.38                 326.38         665.57   \n",
      "44          5.0        306.89                 306.89         640.42   \n",
      "45          NaN        316.14                 316.14         640.42   \n",
      "46          1.0        306.40                 306.40         650.42   \n",
      "47         -7.0        239.27                 239.27         602.74   \n",
      "48          5.0        336.44                 336.44         466.52   \n",
      "49          1.0        336.44                 336.44         462.72   \n",
      "..          ...           ...                    ...            ...   \n",
      "53          0.0        341.68                 341.68         327.31   \n",
      "54          0.0        364.31                 364.31         301.73   \n",
      "55          2.0        360.49                 360.49         305.98   \n",
      "56          1.0        385.31                 385.31         306.89   \n",
      "57          0.0        381.16                 381.16         316.14   \n",
      "58         -2.0        373.96                 373.96         306.40   \n",
      "59          6.0        403.67                 403.67         331.63   \n",
      "60          NaN        410.23                 410.23         336.44   \n",
      "61          4.0        449.10                 449.10         317.94   \n",
      "62         -2.0        462.30                 462.30         303.54   \n",
      "63         -4.0        422.15                 422.15         341.68   \n",
      "64          0.0        422.15                 422.15         341.68   \n",
      "65          0.0        422.15                 422.15         341.68   \n",
      "66          NaN        406.90                 406.90         360.49   \n",
      "67          NaN        406.90                 406.90         360.49   \n",
      "68          7.0        433.41                 433.41         385.31   \n",
      "69          0.0        433.41                 433.41         381.16   \n",
      "70          0.0        453.11                 453.11         373.96   \n",
      "71        -12.0        337.83                 337.83         443.05   \n",
      "72          9.0        466.08                 466.08         410.23   \n",
      "73         -2.0        419.00                 419.00         449.10   \n",
      "74          1.0        452.77                 452.77         462.30   \n",
      "75          3.0        488.00                 488.00         422.15   \n",
      "76          0.0        488.00                 488.00         422.15   \n",
      "77          NaN        488.00                 488.00         422.15   \n",
      "78          1.0        498.63                 498.63         406.90   \n",
      "79          NaN        498.63                 498.63         406.90   \n",
      "80          1.0        485.97                 485.97         433.41   \n",
      "81          0.0        485.97                 485.97         433.41   \n",
      "82          NaN        473.00                 473.00         461.32   \n",
      "\n",
      "    last_year_avg_weighted  two_year_ago_avg  two_year_ago_avg_weighted  \\\n",
      "20                  175.13            360.82                     108.24   \n",
      "21                  175.13            360.82                     108.24   \n",
      "22                  170.63            343.16                     102.95   \n",
      "23                  128.22            323.22                      96.97   \n",
      "24                  128.22            323.22                      96.97   \n",
      "25                  169.76            290.69                      87.21   \n",
      "26                  203.68            298.14                      89.44   \n",
      "27                  207.22            285.53                      85.66   \n",
      "28                  207.22            285.53                      85.66   \n",
      "29                  207.22            285.53                      85.66   \n",
      "30                  181.06            308.64                      92.59   \n",
      "31                  174.16            321.57                      96.47   \n",
      "32                  196.84            350.25                     105.08   \n",
      "33                  196.84            350.25                     105.08   \n",
      "34                  211.61            341.26                     102.38   \n",
      "35                  292.14            256.45                      76.93   \n",
      "36                  324.30            263.45                      79.04   \n",
      "37                  331.37            339.52                     101.86   \n",
      "38                  342.47            407.37                     122.21   \n",
      "39                  327.04            414.44                     124.33   \n",
      "40                  327.04            414.44                     124.33   \n",
      "41                  327.04            414.44                     124.33   \n",
      "42                  347.91            362.11                     108.63   \n",
      "43                  332.78            348.32                     104.50   \n",
      "44                  320.21            393.69                     118.11   \n",
      "45                  320.21            393.69                     118.11   \n",
      "46                  325.21            423.22                     126.97   \n",
      "47                  301.37            521.07                     156.32   \n",
      "48                  233.26            648.60                     194.58   \n",
      "49                  231.36            650.73                     195.22   \n",
      "..                     ...               ...                        ...   \n",
      "53                  163.65            654.09                     196.23   \n",
      "54                  150.86            665.57                     199.67   \n",
      "55                  152.99            695.82                     208.75   \n",
      "56                  153.45            640.42                     192.13   \n",
      "57                  158.07            640.42                     192.13   \n",
      "58                  153.20            650.42                     195.13   \n",
      "59                  165.81            589.24                     176.77   \n",
      "60                  168.22            466.52                     139.96   \n",
      "61                  158.97            444.73                     133.42   \n",
      "62                  151.77            337.65                     101.29   \n",
      "63                  170.84            327.31                      98.19   \n",
      "64                  170.84            327.31                      98.19   \n",
      "65                  170.84            327.31                      98.19   \n",
      "66                  180.24            305.98                      91.79   \n",
      "67                  180.24            326.38                      97.91   \n",
      "68                  192.65            306.89                      92.07   \n",
      "69                  190.58            316.14                      94.84   \n",
      "70                  186.98            306.40                      91.92   \n",
      "71                  221.53            239.27                      71.78   \n",
      "72                  205.12            336.44                     100.93   \n",
      "73                  224.55            317.94                      95.38   \n",
      "74                  231.15            303.54                      91.06   \n",
      "75                  211.07            341.68                     102.50   \n",
      "76                  211.07            341.68                     102.50   \n",
      "77                  211.07            341.68                     102.50   \n",
      "78                  203.45            360.49                     108.15   \n",
      "79                  203.45            360.49                     108.15   \n",
      "80                  216.70            385.31                     115.59   \n",
      "81                  216.70            381.16                     114.35   \n",
      "82                  230.66            349.62                     104.89   \n",
      "\n",
      "    three_year_ago_avg  three_year_ago_avg_weighted confederation rank_date  \n",
      "20              510.95                       102.19      CONCACAF   4/11/13  \n",
      "21              510.95                       102.19      CONCACAF    5/9/13  \n",
      "22              507.42                       101.48      CONCACAF    6/6/13  \n",
      "23              514.63                       102.93      CONCACAF    7/4/13  \n",
      "24              430.01                        86.00      CONCACAF    8/8/13  \n",
      "25              380.25                        76.05      CONCACAF   9/12/13  \n",
      "26              308.72                        61.74      CONCACAF  10/17/13  \n",
      "27              358.73                        71.75      CONCACAF  11/28/13  \n",
      "28              358.73                        71.75      CONCACAF  12/19/13  \n",
      "29              358.73                        71.75      CONCACAF   1/16/14  \n",
      "30              371.15                        74.23      CONCACAF   2/13/14  \n",
      "31              405.60                        81.12      CONCACAF   3/13/14  \n",
      "32              360.82                        72.16      CONCACAF   4/10/14  \n",
      "33              360.82                        72.16      CONCACAF    5/8/14  \n",
      "34              352.07                        70.41      CONCACAF    6/5/14  \n",
      "35              323.22                        64.64      CONCACAF   7/17/14  \n",
      "36              335.41                        67.08      CONCACAF   8/14/14  \n",
      "37              290.69                        58.14      CONCACAF   9/18/14  \n",
      "38              298.14                        59.63      CONCACAF  10/23/14  \n",
      "39              285.53                        57.11      CONCACAF  11/27/14  \n",
      "40              285.53                        57.11      CONCACAF  12/18/14  \n",
      "41              285.53                        57.11      CONCACAF    1/8/15  \n",
      "42              308.64                        61.73      CONCACAF   2/12/15  \n",
      "43              321.57                        64.31      CONCACAF   3/12/15  \n",
      "44              350.25                        70.05      CONCACAF    4/9/15  \n",
      "45              350.25                        70.05      CONCACAF    5/7/15  \n",
      "46              323.30                        64.66      CONCACAF    6/4/15  \n",
      "47              256.45                        51.29      CONCACAF    7/9/15  \n",
      "48              256.45                        51.29      CONCACAF    8/6/15  \n",
      "49              297.77                        59.55      CONCACAF    9/3/15  \n",
      "..                 ...                          ...           ...       ...  \n",
      "53              414.44                        82.89      CONCACAF    1/7/16  \n",
      "54              389.97                        77.99      CONCACAF    2/4/16  \n",
      "55              348.32                        69.66      CONCACAF    3/3/16  \n",
      "56              393.69                        78.74      CONCACAF    4/7/16  \n",
      "57              393.69                        78.74      CONCACAF    5/5/16  \n",
      "58              402.34                        80.47      CONCACAF    6/2/16  \n",
      "59              545.89                       109.18      CONCACAF   7/14/16  \n",
      "60              648.60                       129.72      CONCACAF   8/11/16  \n",
      "61              662.73                       132.55      CONCACAF   9/15/16  \n",
      "62              684.93                       136.99      CONCACAF  10/20/16  \n",
      "63              654.09                       130.82      CONCACAF  11/24/16  \n",
      "64              654.09                       130.82      CONCACAF  12/22/16  \n",
      "65              654.09                       130.82      CONCACAF   1/12/17  \n",
      "66              695.82                       139.16      CONCACAF    2/9/17  \n",
      "67              665.57                       133.11      CONCACAF    3/9/17  \n",
      "68              640.42                       128.08      CONCACAF    4/6/17  \n",
      "69              640.42                       128.08      CONCACAF    5/4/17  \n",
      "70              646.42                       129.28      CONCACAF    6/1/17  \n",
      "71              602.74                       120.55      CONCACAF    7/6/17  \n",
      "72              466.52                        93.30      CONCACAF   8/10/17  \n",
      "73              444.73                        88.95      CONCACAF   9/14/17  \n",
      "74              337.65                        67.53      CONCACAF  10/16/17  \n",
      "75              327.31                        65.46      CONCACAF  11/23/17  \n",
      "76              327.31                        65.46      CONCACAF  12/21/17  \n",
      "77              327.31                        65.46      CONCACAF   1/18/18  \n",
      "78              305.98                        61.20      CONCACAF   2/15/18  \n",
      "79              326.38                        65.28      CONCACAF   3/15/18  \n",
      "80              306.89                        61.38      CONCACAF   4/12/18  \n",
      "81              316.14                        63.23      CONCACAF   5/17/18  \n",
      "82              320.72                        64.14      CONCACAF    6/7/18  \n",
      "\n",
      "[63 rows x 16 columns]\n",
      "\n",
      "Ending File:\n",
      "Empty DataFrame\n",
      "Columns: [rank, country_full, country_abrv, total_points, previous_points, rank_change, cur_year_avg, cur_year_avg_weighted, last_year_avg, last_year_avg_weighted, two_year_ago_avg, two_year_ago_avg_weighted, three_year_ago_avg, three_year_ago_avg_weighted, confederation, rank_date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#path\n",
    "path = \"/Users/petronillagriffith/desktop/Fifa/fifa\"\n",
    "\n",
    "#import file\n",
    "filename = os.path.join(path, \"USA.csv\")\n",
    "\n",
    "#names each column\n",
    "names = ['rank', 'country', 'total_points', 'previous_points', 'rank_change', 'cur_year_avg', \n",
    "          'cur_year_avg_weighted', 'last_year_avg', 'last_year_avg_weighted', 'two_year_ago_avg', \n",
    "          'two_year_ago_avg_weighted', 'three_year_ago_avg_', 'three_year_ago_avg_weighted', 'confederation', 'rank_date']\n",
    "\n",
    "#making a DataFrame\n",
    "df = pd.read_csv(filename, encoding = \"ISO-8859-1\", sep=',', na_values=['-1'], index_col=False)\n",
    "\n",
    "#writes out \"Starting File:\"\n",
    "print(\"Starting File:\")\n",
    "\n",
    "\n",
    "print(df[20:100])\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "#writes out \"Ending File:\"\n",
    "print(\"Ending File:\")\n",
    "\n",
    "print(df[-10:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>country_full</th>\n",
       "      <th>country_abrv</th>\n",
       "      <th>total_points</th>\n",
       "      <th>previous_points</th>\n",
       "      <th>rank_change</th>\n",
       "      <th>cur_year_avg</th>\n",
       "      <th>cur_year_avg_weighted</th>\n",
       "      <th>last_year_avg</th>\n",
       "      <th>last_year_avg_weighted</th>\n",
       "      <th>two_year_ago_avg</th>\n",
       "      <th>two_year_ago_avg_weighted</th>\n",
       "      <th>three_year_ago_avg</th>\n",
       "      <th>three_year_ago_avg_weighted</th>\n",
       "      <th>confederation</th>\n",
       "      <th>rank_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>791.44</td>\n",
       "      <td>777</td>\n",
       "      <td>2.0</td>\n",
       "      <td>335.41</td>\n",
       "      <td>335.41</td>\n",
       "      <td>430.01</td>\n",
       "      <td>215.01</td>\n",
       "      <td>597.61</td>\n",
       "      <td>179.28</td>\n",
       "      <td>308.68</td>\n",
       "      <td>61.74</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>8/24/11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>741.45</td>\n",
       "      <td>791</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>290.69</td>\n",
       "      <td>290.69</td>\n",
       "      <td>380.25</td>\n",
       "      <td>190.13</td>\n",
       "      <td>604.26</td>\n",
       "      <td>181.28</td>\n",
       "      <td>396.74</td>\n",
       "      <td>79.35</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>9/21/11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>719.61</td>\n",
       "      <td>741</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>298.14</td>\n",
       "      <td>298.14</td>\n",
       "      <td>308.72</td>\n",
       "      <td>154.36</td>\n",
       "      <td>634.66</td>\n",
       "      <td>190.40</td>\n",
       "      <td>383.53</td>\n",
       "      <td>76.71</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>10/19/11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>720.85</td>\n",
       "      <td>720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>11/23/11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>720.85</td>\n",
       "      <td>721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>12/21/11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank country_full country_abrv  total_points  previous_points  rank_change  \\\n",
       "0    28          USA          USA        791.44              777          2.0   \n",
       "1    31          USA          USA        741.45              791         -3.0   \n",
       "2    34          USA          USA        719.61              741         -3.0   \n",
       "3    34          USA          USA        720.85              720          0.0   \n",
       "4    34          USA          USA        720.85              721          0.0   \n",
       "\n",
       "   cur_year_avg  cur_year_avg_weighted  last_year_avg  last_year_avg_weighted  \\\n",
       "0        335.41                 335.41         430.01                  215.01   \n",
       "1        290.69                 290.69         380.25                  190.13   \n",
       "2        298.14                 298.14         308.72                  154.36   \n",
       "3        285.53                 285.53         358.73                  179.37   \n",
       "4        285.53                 285.53         358.73                  179.37   \n",
       "\n",
       "   two_year_ago_avg  two_year_ago_avg_weighted  three_year_ago_avg  \\\n",
       "0            597.61                     179.28              308.68   \n",
       "1            604.26                     181.28              396.74   \n",
       "2            634.66                     190.40              383.53   \n",
       "3            583.78                     175.13              404.11   \n",
       "4            583.78                     175.13              404.11   \n",
       "\n",
       "   three_year_ago_avg_weighted confederation rank_date  \n",
       "0                        61.74      CONCACAF   8/24/11  \n",
       "1                        79.35      CONCACAF   9/21/11  \n",
       "2                        76.71      CONCACAF  10/19/11  \n",
       "3                        80.82      CONCACAF  11/23/11  \n",
       "4                        80.82      CONCACAF  12/21/11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['country_full'].unique().tolist()\n",
    "\n",
    "byte_offset_map = {}\n",
    "\n",
    "for i, id in enumerate(a): byte_offset_map[id] = i\n",
    "    \n",
    "df['country_full_class'] = df['country_full'].map(lambda x: byte_offset_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['country_abrv'].unique().tolist()\n",
    "\n",
    "byte_offset_map = {}\n",
    "\n",
    "for i, id in enumerate(a): byte_offset_map[id] = i\n",
    "    \n",
    "df['country_abrv_class'] = df['country_abrv'].map(lambda x: byte_offset_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['confederation'].unique().tolist()\n",
    "\n",
    "byte_offset_map = {}\n",
    "\n",
    "for i, id in enumerate(a): byte_offset_map[id] = i\n",
    "    \n",
    "df['confederation_class'] = df['confederation'].map(lambda x: byte_offset_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['rank_date'].unique().tolist()\n",
    "\n",
    "byte_offset_map = {}\n",
    "\n",
    "for i, id in enumerate(a): byte_offset_map[id] = i\n",
    "    \n",
    "df['rank_date_class'] = df['rank_date'].map(lambda x: byte_offset_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>country_full</th>\n",
       "      <th>country_abrv</th>\n",
       "      <th>total_points</th>\n",
       "      <th>previous_points</th>\n",
       "      <th>rank_change</th>\n",
       "      <th>cur_year_avg</th>\n",
       "      <th>cur_year_avg_weighted</th>\n",
       "      <th>last_year_avg</th>\n",
       "      <th>last_year_avg_weighted</th>\n",
       "      <th>two_year_ago_avg</th>\n",
       "      <th>two_year_ago_avg_weighted</th>\n",
       "      <th>three_year_ago_avg</th>\n",
       "      <th>three_year_ago_avg_weighted</th>\n",
       "      <th>confederation</th>\n",
       "      <th>rank_date</th>\n",
       "      <th>country_full_class</th>\n",
       "      <th>country_abrv_class</th>\n",
       "      <th>confederation_class</th>\n",
       "      <th>rank_date_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>791.44</td>\n",
       "      <td>777</td>\n",
       "      <td>2.0</td>\n",
       "      <td>335.41</td>\n",
       "      <td>335.41</td>\n",
       "      <td>430.01</td>\n",
       "      <td>215.01</td>\n",
       "      <td>597.61</td>\n",
       "      <td>179.28</td>\n",
       "      <td>308.68</td>\n",
       "      <td>61.74</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>8/24/11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>741.45</td>\n",
       "      <td>791</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>290.69</td>\n",
       "      <td>290.69</td>\n",
       "      <td>380.25</td>\n",
       "      <td>190.13</td>\n",
       "      <td>604.26</td>\n",
       "      <td>181.28</td>\n",
       "      <td>396.74</td>\n",
       "      <td>79.35</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>9/21/11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>719.61</td>\n",
       "      <td>741</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>298.14</td>\n",
       "      <td>298.14</td>\n",
       "      <td>308.72</td>\n",
       "      <td>154.36</td>\n",
       "      <td>634.66</td>\n",
       "      <td>190.40</td>\n",
       "      <td>383.53</td>\n",
       "      <td>76.71</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>10/19/11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>720.85</td>\n",
       "      <td>720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>11/23/11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>720.85</td>\n",
       "      <td>721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>12/21/11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>720.85</td>\n",
       "      <td>721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>1/18/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>750.47</td>\n",
       "      <td>721</td>\n",
       "      <td>2.0</td>\n",
       "      <td>308.64</td>\n",
       "      <td>308.64</td>\n",
       "      <td>371.15</td>\n",
       "      <td>185.58</td>\n",
       "      <td>539.98</td>\n",
       "      <td>162.00</td>\n",
       "      <td>471.24</td>\n",
       "      <td>94.25</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>2/15/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>771.60</td>\n",
       "      <td>750</td>\n",
       "      <td>4.0</td>\n",
       "      <td>321.57</td>\n",
       "      <td>321.57</td>\n",
       "      <td>405.60</td>\n",
       "      <td>202.80</td>\n",
       "      <td>509.94</td>\n",
       "      <td>152.98</td>\n",
       "      <td>471.24</td>\n",
       "      <td>94.25</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>3/7/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>778.69</td>\n",
       "      <td>772</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>350.25</td>\n",
       "      <td>350.25</td>\n",
       "      <td>360.82</td>\n",
       "      <td>180.41</td>\n",
       "      <td>510.95</td>\n",
       "      <td>153.29</td>\n",
       "      <td>473.71</td>\n",
       "      <td>94.74</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>4/11/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>778.69</td>\n",
       "      <td>779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350.25</td>\n",
       "      <td>350.25</td>\n",
       "      <td>360.82</td>\n",
       "      <td>180.41</td>\n",
       "      <td>510.95</td>\n",
       "      <td>153.29</td>\n",
       "      <td>473.71</td>\n",
       "      <td>94.74</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>5/9/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>767.03</td>\n",
       "      <td>779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>341.26</td>\n",
       "      <td>341.26</td>\n",
       "      <td>343.16</td>\n",
       "      <td>171.58</td>\n",
       "      <td>484.73</td>\n",
       "      <td>145.42</td>\n",
       "      <td>543.83</td>\n",
       "      <td>108.77</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>6/6/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>686.81</td>\n",
       "      <td>767</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>256.45</td>\n",
       "      <td>256.45</td>\n",
       "      <td>323.22</td>\n",
       "      <td>161.61</td>\n",
       "      <td>499.08</td>\n",
       "      <td>149.72</td>\n",
       "      <td>595.17</td>\n",
       "      <td>119.03</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>7/4/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>36</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>672.13</td>\n",
       "      <td>687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256.45</td>\n",
       "      <td>256.45</td>\n",
       "      <td>323.22</td>\n",
       "      <td>161.61</td>\n",
       "      <td>430.01</td>\n",
       "      <td>129.00</td>\n",
       "      <td>625.33</td>\n",
       "      <td>125.07</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>8/8/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>697.47</td>\n",
       "      <td>672</td>\n",
       "      <td>3.0</td>\n",
       "      <td>297.77</td>\n",
       "      <td>297.77</td>\n",
       "      <td>311.45</td>\n",
       "      <td>155.73</td>\n",
       "      <td>411.07</td>\n",
       "      <td>123.32</td>\n",
       "      <td>603.25</td>\n",
       "      <td>120.65</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>9/5/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>719.79</td>\n",
       "      <td>697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>339.52</td>\n",
       "      <td>339.52</td>\n",
       "      <td>290.69</td>\n",
       "      <td>145.34</td>\n",
       "      <td>380.25</td>\n",
       "      <td>114.08</td>\n",
       "      <td>604.26</td>\n",
       "      <td>120.85</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>10/3/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>775.98</td>\n",
       "      <td>720</td>\n",
       "      <td>5.0</td>\n",
       "      <td>407.37</td>\n",
       "      <td>407.37</td>\n",
       "      <td>298.14</td>\n",
       "      <td>149.07</td>\n",
       "      <td>308.72</td>\n",
       "      <td>92.61</td>\n",
       "      <td>634.66</td>\n",
       "      <td>126.93</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>11/7/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>781.58</td>\n",
       "      <td>776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>414.44</td>\n",
       "      <td>414.44</td>\n",
       "      <td>285.53</td>\n",
       "      <td>142.76</td>\n",
       "      <td>358.73</td>\n",
       "      <td>107.62</td>\n",
       "      <td>583.78</td>\n",
       "      <td>116.76</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>12/19/12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>781.58</td>\n",
       "      <td>782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>414.44</td>\n",
       "      <td>414.44</td>\n",
       "      <td>285.53</td>\n",
       "      <td>142.76</td>\n",
       "      <td>358.73</td>\n",
       "      <td>107.62</td>\n",
       "      <td>583.78</td>\n",
       "      <td>116.76</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>1/17/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>735.78</td>\n",
       "      <td>782</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>362.11</td>\n",
       "      <td>362.11</td>\n",
       "      <td>308.64</td>\n",
       "      <td>154.32</td>\n",
       "      <td>371.15</td>\n",
       "      <td>111.35</td>\n",
       "      <td>539.98</td>\n",
       "      <td>108.00</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>2/14/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>33</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>732.77</td>\n",
       "      <td>736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.32</td>\n",
       "      <td>348.32</td>\n",
       "      <td>321.57</td>\n",
       "      <td>160.78</td>\n",
       "      <td>405.60</td>\n",
       "      <td>121.68</td>\n",
       "      <td>509.94</td>\n",
       "      <td>101.99</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>3/14/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>779.25</td>\n",
       "      <td>733</td>\n",
       "      <td>5.0</td>\n",
       "      <td>393.69</td>\n",
       "      <td>393.69</td>\n",
       "      <td>350.25</td>\n",
       "      <td>175.13</td>\n",
       "      <td>360.82</td>\n",
       "      <td>108.24</td>\n",
       "      <td>510.95</td>\n",
       "      <td>102.19</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>4/11/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>779.25</td>\n",
       "      <td>779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>393.69</td>\n",
       "      <td>393.69</td>\n",
       "      <td>350.25</td>\n",
       "      <td>175.13</td>\n",
       "      <td>360.82</td>\n",
       "      <td>108.24</td>\n",
       "      <td>510.95</td>\n",
       "      <td>102.19</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>5/9/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>798.28</td>\n",
       "      <td>779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>423.22</td>\n",
       "      <td>423.22</td>\n",
       "      <td>341.26</td>\n",
       "      <td>170.63</td>\n",
       "      <td>343.16</td>\n",
       "      <td>102.95</td>\n",
       "      <td>507.42</td>\n",
       "      <td>101.48</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>6/6/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>865.10</td>\n",
       "      <td>798</td>\n",
       "      <td>6.0</td>\n",
       "      <td>536.98</td>\n",
       "      <td>536.98</td>\n",
       "      <td>256.45</td>\n",
       "      <td>128.22</td>\n",
       "      <td>323.22</td>\n",
       "      <td>96.97</td>\n",
       "      <td>514.63</td>\n",
       "      <td>102.93</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>7/4/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>959.80</td>\n",
       "      <td>865</td>\n",
       "      <td>3.0</td>\n",
       "      <td>648.60</td>\n",
       "      <td>648.60</td>\n",
       "      <td>256.45</td>\n",
       "      <td>128.22</td>\n",
       "      <td>323.22</td>\n",
       "      <td>96.97</td>\n",
       "      <td>430.01</td>\n",
       "      <td>86.00</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>8/8/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>995.75</td>\n",
       "      <td>960</td>\n",
       "      <td>6.0</td>\n",
       "      <td>662.73</td>\n",
       "      <td>662.73</td>\n",
       "      <td>339.52</td>\n",
       "      <td>169.76</td>\n",
       "      <td>290.69</td>\n",
       "      <td>87.21</td>\n",
       "      <td>380.25</td>\n",
       "      <td>76.05</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>9/12/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>1039.80</td>\n",
       "      <td>996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>684.93</td>\n",
       "      <td>684.93</td>\n",
       "      <td>407.37</td>\n",
       "      <td>203.68</td>\n",
       "      <td>298.14</td>\n",
       "      <td>89.44</td>\n",
       "      <td>308.72</td>\n",
       "      <td>61.74</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>10/17/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>1018.71</td>\n",
       "      <td>1040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>654.09</td>\n",
       "      <td>654.09</td>\n",
       "      <td>414.44</td>\n",
       "      <td>207.22</td>\n",
       "      <td>285.53</td>\n",
       "      <td>85.66</td>\n",
       "      <td>358.73</td>\n",
       "      <td>71.75</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>11/28/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>1018.71</td>\n",
       "      <td>1019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>654.09</td>\n",
       "      <td>654.09</td>\n",
       "      <td>414.44</td>\n",
       "      <td>207.22</td>\n",
       "      <td>285.53</td>\n",
       "      <td>85.66</td>\n",
       "      <td>358.73</td>\n",
       "      <td>71.75</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>12/19/13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>1018.71</td>\n",
       "      <td>1019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>654.09</td>\n",
       "      <td>654.09</td>\n",
       "      <td>414.44</td>\n",
       "      <td>207.22</td>\n",
       "      <td>285.53</td>\n",
       "      <td>85.66</td>\n",
       "      <td>358.73</td>\n",
       "      <td>71.75</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>1/16/14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>32</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>784.45</td>\n",
       "      <td>784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>341.68</td>\n",
       "      <td>341.68</td>\n",
       "      <td>327.31</td>\n",
       "      <td>163.65</td>\n",
       "      <td>654.09</td>\n",
       "      <td>196.23</td>\n",
       "      <td>414.44</td>\n",
       "      <td>82.89</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>1/7/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>32</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>792.84</td>\n",
       "      <td>784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>364.31</td>\n",
       "      <td>364.31</td>\n",
       "      <td>301.73</td>\n",
       "      <td>150.86</td>\n",
       "      <td>665.57</td>\n",
       "      <td>199.67</td>\n",
       "      <td>389.97</td>\n",
       "      <td>77.99</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>2/4/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>30</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>791.89</td>\n",
       "      <td>793</td>\n",
       "      <td>2.0</td>\n",
       "      <td>360.49</td>\n",
       "      <td>360.49</td>\n",
       "      <td>305.98</td>\n",
       "      <td>152.99</td>\n",
       "      <td>695.82</td>\n",
       "      <td>208.75</td>\n",
       "      <td>348.32</td>\n",
       "      <td>69.66</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>3/3/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>29</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>809.62</td>\n",
       "      <td>792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>385.31</td>\n",
       "      <td>385.31</td>\n",
       "      <td>306.89</td>\n",
       "      <td>153.45</td>\n",
       "      <td>640.42</td>\n",
       "      <td>192.13</td>\n",
       "      <td>393.69</td>\n",
       "      <td>78.74</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>4/7/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>29</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>810.09</td>\n",
       "      <td>810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.16</td>\n",
       "      <td>381.16</td>\n",
       "      <td>316.14</td>\n",
       "      <td>158.07</td>\n",
       "      <td>640.42</td>\n",
       "      <td>192.13</td>\n",
       "      <td>393.69</td>\n",
       "      <td>78.74</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>5/5/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>31</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>802.75</td>\n",
       "      <td>810</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>373.96</td>\n",
       "      <td>373.96</td>\n",
       "      <td>306.40</td>\n",
       "      <td>153.20</td>\n",
       "      <td>650.42</td>\n",
       "      <td>195.13</td>\n",
       "      <td>402.34</td>\n",
       "      <td>80.47</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>6/2/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>25</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>855.43</td>\n",
       "      <td>803</td>\n",
       "      <td>6.0</td>\n",
       "      <td>403.67</td>\n",
       "      <td>403.67</td>\n",
       "      <td>331.63</td>\n",
       "      <td>165.81</td>\n",
       "      <td>589.24</td>\n",
       "      <td>176.77</td>\n",
       "      <td>545.89</td>\n",
       "      <td>109.18</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>7/14/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>26</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>848.13</td>\n",
       "      <td>855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>410.23</td>\n",
       "      <td>410.23</td>\n",
       "      <td>336.44</td>\n",
       "      <td>168.22</td>\n",
       "      <td>466.52</td>\n",
       "      <td>139.96</td>\n",
       "      <td>648.60</td>\n",
       "      <td>129.72</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>8/11/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>22</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>874.03</td>\n",
       "      <td>848</td>\n",
       "      <td>4.0</td>\n",
       "      <td>449.10</td>\n",
       "      <td>449.10</td>\n",
       "      <td>317.94</td>\n",
       "      <td>158.97</td>\n",
       "      <td>444.73</td>\n",
       "      <td>133.42</td>\n",
       "      <td>662.73</td>\n",
       "      <td>132.55</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>9/15/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>24</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>852.35</td>\n",
       "      <td>874</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>462.30</td>\n",
       "      <td>462.30</td>\n",
       "      <td>303.54</td>\n",
       "      <td>151.77</td>\n",
       "      <td>337.65</td>\n",
       "      <td>101.29</td>\n",
       "      <td>684.93</td>\n",
       "      <td>136.99</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>10/20/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>822.00</td>\n",
       "      <td>852</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>422.15</td>\n",
       "      <td>422.15</td>\n",
       "      <td>341.68</td>\n",
       "      <td>170.84</td>\n",
       "      <td>327.31</td>\n",
       "      <td>98.19</td>\n",
       "      <td>654.09</td>\n",
       "      <td>130.82</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>11/24/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>822.00</td>\n",
       "      <td>822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>422.15</td>\n",
       "      <td>422.15</td>\n",
       "      <td>341.68</td>\n",
       "      <td>170.84</td>\n",
       "      <td>327.31</td>\n",
       "      <td>98.19</td>\n",
       "      <td>654.09</td>\n",
       "      <td>130.82</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>12/22/16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>822.00</td>\n",
       "      <td>822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>422.15</td>\n",
       "      <td>422.15</td>\n",
       "      <td>341.68</td>\n",
       "      <td>170.84</td>\n",
       "      <td>327.31</td>\n",
       "      <td>98.19</td>\n",
       "      <td>654.09</td>\n",
       "      <td>130.82</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>1/12/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>29</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>818.10</td>\n",
       "      <td>822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.90</td>\n",
       "      <td>406.90</td>\n",
       "      <td>360.49</td>\n",
       "      <td>180.24</td>\n",
       "      <td>305.98</td>\n",
       "      <td>91.79</td>\n",
       "      <td>695.82</td>\n",
       "      <td>139.16</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>2/9/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>30</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>818.17</td>\n",
       "      <td>818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.90</td>\n",
       "      <td>406.90</td>\n",
       "      <td>360.49</td>\n",
       "      <td>180.24</td>\n",
       "      <td>326.38</td>\n",
       "      <td>97.91</td>\n",
       "      <td>665.57</td>\n",
       "      <td>133.11</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>3/9/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>23</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>846.21</td>\n",
       "      <td>818</td>\n",
       "      <td>7.0</td>\n",
       "      <td>433.41</td>\n",
       "      <td>433.41</td>\n",
       "      <td>385.31</td>\n",
       "      <td>192.65</td>\n",
       "      <td>306.89</td>\n",
       "      <td>92.07</td>\n",
       "      <td>640.42</td>\n",
       "      <td>128.08</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>4/6/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>23</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>846.91</td>\n",
       "      <td>846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>433.41</td>\n",
       "      <td>433.41</td>\n",
       "      <td>381.16</td>\n",
       "      <td>190.58</td>\n",
       "      <td>316.14</td>\n",
       "      <td>94.84</td>\n",
       "      <td>640.42</td>\n",
       "      <td>128.08</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>5/4/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>23</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>861.29</td>\n",
       "      <td>847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>453.11</td>\n",
       "      <td>453.11</td>\n",
       "      <td>373.96</td>\n",
       "      <td>186.98</td>\n",
       "      <td>306.40</td>\n",
       "      <td>91.92</td>\n",
       "      <td>646.42</td>\n",
       "      <td>129.28</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>6/1/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>35</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>751.69</td>\n",
       "      <td>861</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>337.83</td>\n",
       "      <td>337.83</td>\n",
       "      <td>443.05</td>\n",
       "      <td>221.53</td>\n",
       "      <td>239.27</td>\n",
       "      <td>71.78</td>\n",
       "      <td>602.74</td>\n",
       "      <td>120.55</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>7/6/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>26</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>865.43</td>\n",
       "      <td>752</td>\n",
       "      <td>9.0</td>\n",
       "      <td>466.08</td>\n",
       "      <td>466.08</td>\n",
       "      <td>410.23</td>\n",
       "      <td>205.12</td>\n",
       "      <td>336.44</td>\n",
       "      <td>100.93</td>\n",
       "      <td>466.52</td>\n",
       "      <td>93.30</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>8/10/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>28</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>827.87</td>\n",
       "      <td>865</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>419.00</td>\n",
       "      <td>419.00</td>\n",
       "      <td>449.10</td>\n",
       "      <td>224.55</td>\n",
       "      <td>317.94</td>\n",
       "      <td>95.38</td>\n",
       "      <td>444.73</td>\n",
       "      <td>88.95</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>9/14/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>27</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>842.51</td>\n",
       "      <td>828</td>\n",
       "      <td>1.0</td>\n",
       "      <td>452.77</td>\n",
       "      <td>452.77</td>\n",
       "      <td>462.30</td>\n",
       "      <td>231.15</td>\n",
       "      <td>303.54</td>\n",
       "      <td>91.06</td>\n",
       "      <td>337.65</td>\n",
       "      <td>67.53</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>10/16/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>24</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>867.03</td>\n",
       "      <td>843</td>\n",
       "      <td>3.0</td>\n",
       "      <td>488.00</td>\n",
       "      <td>488.00</td>\n",
       "      <td>422.15</td>\n",
       "      <td>211.07</td>\n",
       "      <td>341.68</td>\n",
       "      <td>102.50</td>\n",
       "      <td>327.31</td>\n",
       "      <td>65.46</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>11/23/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>24</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>867.03</td>\n",
       "      <td>867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>488.00</td>\n",
       "      <td>488.00</td>\n",
       "      <td>422.15</td>\n",
       "      <td>211.07</td>\n",
       "      <td>341.68</td>\n",
       "      <td>102.50</td>\n",
       "      <td>327.31</td>\n",
       "      <td>65.46</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>12/21/17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>25</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>867.03</td>\n",
       "      <td>867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>488.00</td>\n",
       "      <td>488.00</td>\n",
       "      <td>422.15</td>\n",
       "      <td>211.07</td>\n",
       "      <td>341.68</td>\n",
       "      <td>102.50</td>\n",
       "      <td>327.31</td>\n",
       "      <td>65.46</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>1/18/18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>24</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>871.42</td>\n",
       "      <td>867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>498.63</td>\n",
       "      <td>498.63</td>\n",
       "      <td>406.90</td>\n",
       "      <td>203.45</td>\n",
       "      <td>360.49</td>\n",
       "      <td>108.15</td>\n",
       "      <td>305.98</td>\n",
       "      <td>61.20</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>2/15/18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>25</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>875.50</td>\n",
       "      <td>871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>498.63</td>\n",
       "      <td>498.63</td>\n",
       "      <td>406.90</td>\n",
       "      <td>203.45</td>\n",
       "      <td>360.49</td>\n",
       "      <td>108.15</td>\n",
       "      <td>326.38</td>\n",
       "      <td>65.28</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>3/15/18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>24</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>879.64</td>\n",
       "      <td>876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>485.97</td>\n",
       "      <td>485.97</td>\n",
       "      <td>433.41</td>\n",
       "      <td>216.70</td>\n",
       "      <td>385.31</td>\n",
       "      <td>115.59</td>\n",
       "      <td>306.89</td>\n",
       "      <td>61.38</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>4/12/18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>24</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>880.25</td>\n",
       "      <td>880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>485.97</td>\n",
       "      <td>485.97</td>\n",
       "      <td>433.41</td>\n",
       "      <td>216.70</td>\n",
       "      <td>381.16</td>\n",
       "      <td>114.35</td>\n",
       "      <td>316.14</td>\n",
       "      <td>63.23</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>5/17/18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>25</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>872.68</td>\n",
       "      <td>880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>473.00</td>\n",
       "      <td>473.00</td>\n",
       "      <td>461.32</td>\n",
       "      <td>230.66</td>\n",
       "      <td>349.62</td>\n",
       "      <td>104.89</td>\n",
       "      <td>320.72</td>\n",
       "      <td>64.14</td>\n",
       "      <td>CONCACAF</td>\n",
       "      <td>6/7/18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank country_full country_abrv  total_points  previous_points  \\\n",
       "0     28          USA          USA        791.44              777   \n",
       "1     31          USA          USA        741.45              791   \n",
       "2     34          USA          USA        719.61              741   \n",
       "3     34          USA          USA        720.85              720   \n",
       "4     34          USA          USA        720.85              721   \n",
       "5     33          USA          USA        720.85              721   \n",
       "6     31          USA          USA        750.47              721   \n",
       "7     27          USA          USA        771.60              750   \n",
       "8     29          USA          USA        778.69              772   \n",
       "9     29          USA          USA        778.69              779   \n",
       "10    28          USA          USA        767.03              779   \n",
       "11    36          USA          USA        686.81              767   \n",
       "12    36          USA          USA        672.13              687   \n",
       "13    33          USA          USA        697.47              672   \n",
       "14    32          USA          USA        719.79              697   \n",
       "15    27          USA          USA        775.98              720   \n",
       "16    28          USA          USA        781.58              776   \n",
       "17    28          USA          USA        781.58              782   \n",
       "18    32          USA          USA        735.78              782   \n",
       "19    33          USA          USA        732.77              736   \n",
       "20    28          USA          USA        779.25              733   \n",
       "21    29          USA          USA        779.25              779   \n",
       "22    28          USA          USA        798.28              779   \n",
       "23    22          USA          USA        865.10              798   \n",
       "24    19          USA          USA        959.80              865   \n",
       "25    13          USA          USA        995.75              960   \n",
       "26    13          USA          USA       1039.80              996   \n",
       "27    14          USA          USA       1018.71             1040   \n",
       "28    14          USA          USA       1018.71             1019   \n",
       "29    14          USA          USA       1018.71             1019   \n",
       "..   ...          ...          ...           ...              ...   \n",
       "53    32          USA          USA        784.45              784   \n",
       "54    32          USA          USA        792.84              784   \n",
       "55    30          USA          USA        791.89              793   \n",
       "56    29          USA          USA        809.62              792   \n",
       "57    29          USA          USA        810.09              810   \n",
       "58    31          USA          USA        802.75              810   \n",
       "59    25          USA          USA        855.43              803   \n",
       "60    26          USA          USA        848.13              855   \n",
       "61    22          USA          USA        874.03              848   \n",
       "62    24          USA          USA        852.35              874   \n",
       "63    28          USA          USA        822.00              852   \n",
       "64    28          USA          USA        822.00              822   \n",
       "65    28          USA          USA        822.00              822   \n",
       "66    29          USA          USA        818.10              822   \n",
       "67    30          USA          USA        818.17              818   \n",
       "68    23          USA          USA        846.21              818   \n",
       "69    23          USA          USA        846.91              846   \n",
       "70    23          USA          USA        861.29              847   \n",
       "71    35          USA          USA        751.69              861   \n",
       "72    26          USA          USA        865.43              752   \n",
       "73    28          USA          USA        827.87              865   \n",
       "74    27          USA          USA        842.51              828   \n",
       "75    24          USA          USA        867.03              843   \n",
       "76    24          USA          USA        867.03              867   \n",
       "77    25          USA          USA        867.03              867   \n",
       "78    24          USA          USA        871.42              867   \n",
       "79    25          USA          USA        875.50              871   \n",
       "80    24          USA          USA        879.64              876   \n",
       "81    24          USA          USA        880.25              880   \n",
       "82    25          USA          USA        872.68              880   \n",
       "\n",
       "    rank_change  cur_year_avg  cur_year_avg_weighted  last_year_avg  \\\n",
       "0           2.0        335.41                 335.41         430.01   \n",
       "1          -3.0        290.69                 290.69         380.25   \n",
       "2          -3.0        298.14                 298.14         308.72   \n",
       "3           0.0        285.53                 285.53         358.73   \n",
       "4           0.0        285.53                 285.53         358.73   \n",
       "5           1.0        285.53                 285.53         358.73   \n",
       "6           2.0        308.64                 308.64         371.15   \n",
       "7           4.0        321.57                 321.57         405.60   \n",
       "8          -2.0        350.25                 350.25         360.82   \n",
       "9           0.0        350.25                 350.25         360.82   \n",
       "10          1.0        341.26                 341.26         343.16   \n",
       "11         -8.0        256.45                 256.45         323.22   \n",
       "12          0.0        256.45                 256.45         323.22   \n",
       "13          3.0        297.77                 297.77         311.45   \n",
       "14          1.0        339.52                 339.52         290.69   \n",
       "15          5.0        407.37                 407.37         298.14   \n",
       "16          NaN        414.44                 414.44         285.53   \n",
       "17          0.0        414.44                 414.44         285.53   \n",
       "18         -4.0        362.11                 362.11         308.64   \n",
       "19          NaN        348.32                 348.32         321.57   \n",
       "20          5.0        393.69                 393.69         350.25   \n",
       "21          NaN        393.69                 393.69         350.25   \n",
       "22          1.0        423.22                 423.22         341.26   \n",
       "23          6.0        536.98                 536.98         256.45   \n",
       "24          3.0        648.60                 648.60         256.45   \n",
       "25          6.0        662.73                 662.73         339.52   \n",
       "26          0.0        684.93                 684.93         407.37   \n",
       "27          NaN        654.09                 654.09         414.44   \n",
       "28          0.0        654.09                 654.09         414.44   \n",
       "29          0.0        654.09                 654.09         414.44   \n",
       "..          ...           ...                    ...            ...   \n",
       "53          0.0        341.68                 341.68         327.31   \n",
       "54          0.0        364.31                 364.31         301.73   \n",
       "55          2.0        360.49                 360.49         305.98   \n",
       "56          1.0        385.31                 385.31         306.89   \n",
       "57          0.0        381.16                 381.16         316.14   \n",
       "58         -2.0        373.96                 373.96         306.40   \n",
       "59          6.0        403.67                 403.67         331.63   \n",
       "60          NaN        410.23                 410.23         336.44   \n",
       "61          4.0        449.10                 449.10         317.94   \n",
       "62         -2.0        462.30                 462.30         303.54   \n",
       "63         -4.0        422.15                 422.15         341.68   \n",
       "64          0.0        422.15                 422.15         341.68   \n",
       "65          0.0        422.15                 422.15         341.68   \n",
       "66          NaN        406.90                 406.90         360.49   \n",
       "67          NaN        406.90                 406.90         360.49   \n",
       "68          7.0        433.41                 433.41         385.31   \n",
       "69          0.0        433.41                 433.41         381.16   \n",
       "70          0.0        453.11                 453.11         373.96   \n",
       "71        -12.0        337.83                 337.83         443.05   \n",
       "72          9.0        466.08                 466.08         410.23   \n",
       "73         -2.0        419.00                 419.00         449.10   \n",
       "74          1.0        452.77                 452.77         462.30   \n",
       "75          3.0        488.00                 488.00         422.15   \n",
       "76          0.0        488.00                 488.00         422.15   \n",
       "77          NaN        488.00                 488.00         422.15   \n",
       "78          1.0        498.63                 498.63         406.90   \n",
       "79          NaN        498.63                 498.63         406.90   \n",
       "80          1.0        485.97                 485.97         433.41   \n",
       "81          0.0        485.97                 485.97         433.41   \n",
       "82          NaN        473.00                 473.00         461.32   \n",
       "\n",
       "    last_year_avg_weighted  two_year_ago_avg  two_year_ago_avg_weighted  \\\n",
       "0                   215.01            597.61                     179.28   \n",
       "1                   190.13            604.26                     181.28   \n",
       "2                   154.36            634.66                     190.40   \n",
       "3                   179.37            583.78                     175.13   \n",
       "4                   179.37            583.78                     175.13   \n",
       "5                   179.37            583.78                     175.13   \n",
       "6                   185.58            539.98                     162.00   \n",
       "7                   202.80            509.94                     152.98   \n",
       "8                   180.41            510.95                     153.29   \n",
       "9                   180.41            510.95                     153.29   \n",
       "10                  171.58            484.73                     145.42   \n",
       "11                  161.61            499.08                     149.72   \n",
       "12                  161.61            430.01                     129.00   \n",
       "13                  155.73            411.07                     123.32   \n",
       "14                  145.34            380.25                     114.08   \n",
       "15                  149.07            308.72                      92.61   \n",
       "16                  142.76            358.73                     107.62   \n",
       "17                  142.76            358.73                     107.62   \n",
       "18                  154.32            371.15                     111.35   \n",
       "19                  160.78            405.60                     121.68   \n",
       "20                  175.13            360.82                     108.24   \n",
       "21                  175.13            360.82                     108.24   \n",
       "22                  170.63            343.16                     102.95   \n",
       "23                  128.22            323.22                      96.97   \n",
       "24                  128.22            323.22                      96.97   \n",
       "25                  169.76            290.69                      87.21   \n",
       "26                  203.68            298.14                      89.44   \n",
       "27                  207.22            285.53                      85.66   \n",
       "28                  207.22            285.53                      85.66   \n",
       "29                  207.22            285.53                      85.66   \n",
       "..                     ...               ...                        ...   \n",
       "53                  163.65            654.09                     196.23   \n",
       "54                  150.86            665.57                     199.67   \n",
       "55                  152.99            695.82                     208.75   \n",
       "56                  153.45            640.42                     192.13   \n",
       "57                  158.07            640.42                     192.13   \n",
       "58                  153.20            650.42                     195.13   \n",
       "59                  165.81            589.24                     176.77   \n",
       "60                  168.22            466.52                     139.96   \n",
       "61                  158.97            444.73                     133.42   \n",
       "62                  151.77            337.65                     101.29   \n",
       "63                  170.84            327.31                      98.19   \n",
       "64                  170.84            327.31                      98.19   \n",
       "65                  170.84            327.31                      98.19   \n",
       "66                  180.24            305.98                      91.79   \n",
       "67                  180.24            326.38                      97.91   \n",
       "68                  192.65            306.89                      92.07   \n",
       "69                  190.58            316.14                      94.84   \n",
       "70                  186.98            306.40                      91.92   \n",
       "71                  221.53            239.27                      71.78   \n",
       "72                  205.12            336.44                     100.93   \n",
       "73                  224.55            317.94                      95.38   \n",
       "74                  231.15            303.54                      91.06   \n",
       "75                  211.07            341.68                     102.50   \n",
       "76                  211.07            341.68                     102.50   \n",
       "77                  211.07            341.68                     102.50   \n",
       "78                  203.45            360.49                     108.15   \n",
       "79                  203.45            360.49                     108.15   \n",
       "80                  216.70            385.31                     115.59   \n",
       "81                  216.70            381.16                     114.35   \n",
       "82                  230.66            349.62                     104.89   \n",
       "\n",
       "    three_year_ago_avg  three_year_ago_avg_weighted confederation rank_date  \\\n",
       "0               308.68                        61.74      CONCACAF   8/24/11   \n",
       "1               396.74                        79.35      CONCACAF   9/21/11   \n",
       "2               383.53                        76.71      CONCACAF  10/19/11   \n",
       "3               404.11                        80.82      CONCACAF  11/23/11   \n",
       "4               404.11                        80.82      CONCACAF  12/21/11   \n",
       "5               404.11                        80.82      CONCACAF   1/18/12   \n",
       "6               471.24                        94.25      CONCACAF   2/15/12   \n",
       "7               471.24                        94.25      CONCACAF    3/7/12   \n",
       "8               473.71                        94.74      CONCACAF   4/11/12   \n",
       "9               473.71                        94.74      CONCACAF    5/9/12   \n",
       "10              543.83                       108.77      CONCACAF    6/6/12   \n",
       "11              595.17                       119.03      CONCACAF    7/4/12   \n",
       "12              625.33                       125.07      CONCACAF    8/8/12   \n",
       "13              603.25                       120.65      CONCACAF    9/5/12   \n",
       "14              604.26                       120.85      CONCACAF   10/3/12   \n",
       "15              634.66                       126.93      CONCACAF   11/7/12   \n",
       "16              583.78                       116.76      CONCACAF  12/19/12   \n",
       "17              583.78                       116.76      CONCACAF   1/17/13   \n",
       "18              539.98                       108.00      CONCACAF   2/14/13   \n",
       "19              509.94                       101.99      CONCACAF   3/14/13   \n",
       "20              510.95                       102.19      CONCACAF   4/11/13   \n",
       "21              510.95                       102.19      CONCACAF    5/9/13   \n",
       "22              507.42                       101.48      CONCACAF    6/6/13   \n",
       "23              514.63                       102.93      CONCACAF    7/4/13   \n",
       "24              430.01                        86.00      CONCACAF    8/8/13   \n",
       "25              380.25                        76.05      CONCACAF   9/12/13   \n",
       "26              308.72                        61.74      CONCACAF  10/17/13   \n",
       "27              358.73                        71.75      CONCACAF  11/28/13   \n",
       "28              358.73                        71.75      CONCACAF  12/19/13   \n",
       "29              358.73                        71.75      CONCACAF   1/16/14   \n",
       "..                 ...                          ...           ...       ...   \n",
       "53              414.44                        82.89      CONCACAF    1/7/16   \n",
       "54              389.97                        77.99      CONCACAF    2/4/16   \n",
       "55              348.32                        69.66      CONCACAF    3/3/16   \n",
       "56              393.69                        78.74      CONCACAF    4/7/16   \n",
       "57              393.69                        78.74      CONCACAF    5/5/16   \n",
       "58              402.34                        80.47      CONCACAF    6/2/16   \n",
       "59              545.89                       109.18      CONCACAF   7/14/16   \n",
       "60              648.60                       129.72      CONCACAF   8/11/16   \n",
       "61              662.73                       132.55      CONCACAF   9/15/16   \n",
       "62              684.93                       136.99      CONCACAF  10/20/16   \n",
       "63              654.09                       130.82      CONCACAF  11/24/16   \n",
       "64              654.09                       130.82      CONCACAF  12/22/16   \n",
       "65              654.09                       130.82      CONCACAF   1/12/17   \n",
       "66              695.82                       139.16      CONCACAF    2/9/17   \n",
       "67              665.57                       133.11      CONCACAF    3/9/17   \n",
       "68              640.42                       128.08      CONCACAF    4/6/17   \n",
       "69              640.42                       128.08      CONCACAF    5/4/17   \n",
       "70              646.42                       129.28      CONCACAF    6/1/17   \n",
       "71              602.74                       120.55      CONCACAF    7/6/17   \n",
       "72              466.52                        93.30      CONCACAF   8/10/17   \n",
       "73              444.73                        88.95      CONCACAF   9/14/17   \n",
       "74              337.65                        67.53      CONCACAF  10/16/17   \n",
       "75              327.31                        65.46      CONCACAF  11/23/17   \n",
       "76              327.31                        65.46      CONCACAF  12/21/17   \n",
       "77              327.31                        65.46      CONCACAF   1/18/18   \n",
       "78              305.98                        61.20      CONCACAF   2/15/18   \n",
       "79              326.38                        65.28      CONCACAF   3/15/18   \n",
       "80              306.89                        61.38      CONCACAF   4/12/18   \n",
       "81              316.14                        63.23      CONCACAF   5/17/18   \n",
       "82              320.72                        64.14      CONCACAF    6/7/18   \n",
       "\n",
       "    country_full_class  country_abrv_class  confederation_class  \\\n",
       "0                    0                   0                    0   \n",
       "1                    0                   0                    0   \n",
       "2                    0                   0                    0   \n",
       "3                    0                   0                    0   \n",
       "4                    0                   0                    0   \n",
       "5                    0                   0                    0   \n",
       "6                    0                   0                    0   \n",
       "7                    0                   0                    0   \n",
       "8                    0                   0                    0   \n",
       "9                    0                   0                    0   \n",
       "10                   0                   0                    0   \n",
       "11                   0                   0                    0   \n",
       "12                   0                   0                    0   \n",
       "13                   0                   0                    0   \n",
       "14                   0                   0                    0   \n",
       "15                   0                   0                    0   \n",
       "16                   0                   0                    0   \n",
       "17                   0                   0                    0   \n",
       "18                   0                   0                    0   \n",
       "19                   0                   0                    0   \n",
       "20                   0                   0                    0   \n",
       "21                   0                   0                    0   \n",
       "22                   0                   0                    0   \n",
       "23                   0                   0                    0   \n",
       "24                   0                   0                    0   \n",
       "25                   0                   0                    0   \n",
       "26                   0                   0                    0   \n",
       "27                   0                   0                    0   \n",
       "28                   0                   0                    0   \n",
       "29                   0                   0                    0   \n",
       "..                 ...                 ...                  ...   \n",
       "53                   0                   0                    0   \n",
       "54                   0                   0                    0   \n",
       "55                   0                   0                    0   \n",
       "56                   0                   0                    0   \n",
       "57                   0                   0                    0   \n",
       "58                   0                   0                    0   \n",
       "59                   0                   0                    0   \n",
       "60                   0                   0                    0   \n",
       "61                   0                   0                    0   \n",
       "62                   0                   0                    0   \n",
       "63                   0                   0                    0   \n",
       "64                   0                   0                    0   \n",
       "65                   0                   0                    0   \n",
       "66                   0                   0                    0   \n",
       "67                   0                   0                    0   \n",
       "68                   0                   0                    0   \n",
       "69                   0                   0                    0   \n",
       "70                   0                   0                    0   \n",
       "71                   0                   0                    0   \n",
       "72                   0                   0                    0   \n",
       "73                   0                   0                    0   \n",
       "74                   0                   0                    0   \n",
       "75                   0                   0                    0   \n",
       "76                   0                   0                    0   \n",
       "77                   0                   0                    0   \n",
       "78                   0                   0                    0   \n",
       "79                   0                   0                    0   \n",
       "80                   0                   0                    0   \n",
       "81                   0                   0                    0   \n",
       "82                   0                   0                    0   \n",
       "\n",
       "    rank_date_class  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 2  \n",
       "3                 3  \n",
       "4                 4  \n",
       "5                 5  \n",
       "6                 6  \n",
       "7                 7  \n",
       "8                 8  \n",
       "9                 9  \n",
       "10               10  \n",
       "11               11  \n",
       "12               12  \n",
       "13               13  \n",
       "14               14  \n",
       "15               15  \n",
       "16               16  \n",
       "17               17  \n",
       "18               18  \n",
       "19               19  \n",
       "20               20  \n",
       "21               21  \n",
       "22               22  \n",
       "23               23  \n",
       "24               24  \n",
       "25               25  \n",
       "26               26  \n",
       "27               27  \n",
       "28               28  \n",
       "29               29  \n",
       "..              ...  \n",
       "53               53  \n",
       "54               54  \n",
       "55               55  \n",
       "56               56  \n",
       "57               57  \n",
       "58               58  \n",
       "59               59  \n",
       "60               60  \n",
       "61               61  \n",
       "62               62  \n",
       "63               63  \n",
       "64               64  \n",
       "65               65  \n",
       "66               66  \n",
       "67               67  \n",
       "68               68  \n",
       "69               69  \n",
       "70               70  \n",
       "71               71  \n",
       "72               72  \n",
       "73               73  \n",
       "74               74  \n",
       "75               75  \n",
       "76               76  \n",
       "77               77  \n",
       "78               78  \n",
       "79               79  \n",
       "80               80  \n",
       "81               81  \n",
       "82               82  \n",
       "\n",
       "[83 rows x 20 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops the 'country' column from the data set\n",
    "df = df.drop(['country_full'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops the 'country' column from the data set\n",
    "df = df.drop(['country_abrv'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops the 'confederation' column from the data set\n",
    "df = df.drop(['confederation'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops the 'confederation' column from the data set\n",
    "df = df.drop(['rank_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>total_points</th>\n",
       "      <th>previous_points</th>\n",
       "      <th>rank_change</th>\n",
       "      <th>cur_year_avg</th>\n",
       "      <th>cur_year_avg_weighted</th>\n",
       "      <th>last_year_avg</th>\n",
       "      <th>last_year_avg_weighted</th>\n",
       "      <th>two_year_ago_avg</th>\n",
       "      <th>two_year_ago_avg_weighted</th>\n",
       "      <th>three_year_ago_avg</th>\n",
       "      <th>three_year_ago_avg_weighted</th>\n",
       "      <th>country_full_class</th>\n",
       "      <th>country_abrv_class</th>\n",
       "      <th>confederation_class</th>\n",
       "      <th>rank_date_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>791.44</td>\n",
       "      <td>777</td>\n",
       "      <td>2.0</td>\n",
       "      <td>335.41</td>\n",
       "      <td>335.41</td>\n",
       "      <td>430.01</td>\n",
       "      <td>215.01</td>\n",
       "      <td>597.61</td>\n",
       "      <td>179.28</td>\n",
       "      <td>308.68</td>\n",
       "      <td>61.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>741.45</td>\n",
       "      <td>791</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>290.69</td>\n",
       "      <td>290.69</td>\n",
       "      <td>380.25</td>\n",
       "      <td>190.13</td>\n",
       "      <td>604.26</td>\n",
       "      <td>181.28</td>\n",
       "      <td>396.74</td>\n",
       "      <td>79.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>719.61</td>\n",
       "      <td>741</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>298.14</td>\n",
       "      <td>298.14</td>\n",
       "      <td>308.72</td>\n",
       "      <td>154.36</td>\n",
       "      <td>634.66</td>\n",
       "      <td>190.40</td>\n",
       "      <td>383.53</td>\n",
       "      <td>76.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>720.85</td>\n",
       "      <td>720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>720.85</td>\n",
       "      <td>721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  total_points  previous_points  rank_change  cur_year_avg  \\\n",
       "0    28        791.44              777          2.0        335.41   \n",
       "1    31        741.45              791         -3.0        290.69   \n",
       "2    34        719.61              741         -3.0        298.14   \n",
       "3    34        720.85              720          0.0        285.53   \n",
       "4    34        720.85              721          0.0        285.53   \n",
       "\n",
       "   cur_year_avg_weighted  last_year_avg  last_year_avg_weighted  \\\n",
       "0                 335.41         430.01                  215.01   \n",
       "1                 290.69         380.25                  190.13   \n",
       "2                 298.14         308.72                  154.36   \n",
       "3                 285.53         358.73                  179.37   \n",
       "4                 285.53         358.73                  179.37   \n",
       "\n",
       "   two_year_ago_avg  two_year_ago_avg_weighted  three_year_ago_avg  \\\n",
       "0            597.61                     179.28              308.68   \n",
       "1            604.26                     181.28              396.74   \n",
       "2            634.66                     190.40              383.53   \n",
       "3            583.78                     175.13              404.11   \n",
       "4            583.78                     175.13              404.11   \n",
       "\n",
       "   three_year_ago_avg_weighted  country_full_class  country_abrv_class  \\\n",
       "0                        61.74                   0                   0   \n",
       "1                        79.35                   0                   0   \n",
       "2                        76.71                   0                   0   \n",
       "3                        80.82                   0                   0   \n",
       "4                        80.82                   0                   0   \n",
       "\n",
       "   confederation_class  rank_date_class  \n",
       "0                    0                0  \n",
       "1                    0                1  \n",
       "2                    0                2  \n",
       "3                    0                3  \n",
       "4                    0                4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = df.loc[df['country_full_class'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_country = max['country_full_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rank                             int64\n",
       "total_points                   float64\n",
       "previous_points                  int64\n",
       "rank_change                    float64\n",
       "cur_year_avg                   float64\n",
       "cur_year_avg_weighted          float64\n",
       "last_year_avg                  float64\n",
       "last_year_avg_weighted         float64\n",
       "two_year_ago_avg               float64\n",
       "two_year_ago_avg_weighted      float64\n",
       "three_year_ago_avg             float64\n",
       "three_year_ago_avg_weighted    float64\n",
       "country_full_class               int64\n",
       "country_abrv_class               int64\n",
       "confederation_class              int64\n",
       "rank_date_class                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, y = to_xy(df,'total_points')\n",
    "\n",
    "#y = df['total_points']\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "#print(x_train.shape)\n",
    "#print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "x = int(len(df)*.7)\n",
    "pt_to_split = int(x)\n",
    "print(pt_to_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 16)\n",
      "(25, 16)\n"
     ]
    }
   ],
   "source": [
    "x = df\n",
    "x_train= df[:pt_to_split]\n",
    "x_test = df[pt_to_split:]\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58,)\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "y = df['total_points']\n",
    "y_train = y[:pt_to_split]\n",
    "y_test = y [pt_to_split:]\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['rank_change'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>total_points</th>\n",
       "      <th>previous_points</th>\n",
       "      <th>rank_change</th>\n",
       "      <th>cur_year_avg</th>\n",
       "      <th>cur_year_avg_weighted</th>\n",
       "      <th>last_year_avg</th>\n",
       "      <th>last_year_avg_weighted</th>\n",
       "      <th>two_year_ago_avg</th>\n",
       "      <th>two_year_ago_avg_weighted</th>\n",
       "      <th>three_year_ago_avg</th>\n",
       "      <th>three_year_ago_avg_weighted</th>\n",
       "      <th>country_full_class</th>\n",
       "      <th>country_abrv_class</th>\n",
       "      <th>confederation_class</th>\n",
       "      <th>rank_date_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28</td>\n",
       "      <td>781.58</td>\n",
       "      <td>776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>414.44</td>\n",
       "      <td>414.44</td>\n",
       "      <td>285.53</td>\n",
       "      <td>142.76</td>\n",
       "      <td>358.73</td>\n",
       "      <td>107.62</td>\n",
       "      <td>583.78</td>\n",
       "      <td>116.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>33</td>\n",
       "      <td>732.77</td>\n",
       "      <td>736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.32</td>\n",
       "      <td>348.32</td>\n",
       "      <td>321.57</td>\n",
       "      <td>160.78</td>\n",
       "      <td>405.60</td>\n",
       "      <td>121.68</td>\n",
       "      <td>509.94</td>\n",
       "      <td>101.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29</td>\n",
       "      <td>779.25</td>\n",
       "      <td>779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>393.69</td>\n",
       "      <td>393.69</td>\n",
       "      <td>350.25</td>\n",
       "      <td>175.13</td>\n",
       "      <td>360.82</td>\n",
       "      <td>108.24</td>\n",
       "      <td>510.95</td>\n",
       "      <td>102.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>1018.71</td>\n",
       "      <td>1040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>654.09</td>\n",
       "      <td>654.09</td>\n",
       "      <td>414.44</td>\n",
       "      <td>207.22</td>\n",
       "      <td>285.53</td>\n",
       "      <td>85.66</td>\n",
       "      <td>358.73</td>\n",
       "      <td>71.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14</td>\n",
       "      <td>1017.32</td>\n",
       "      <td>1044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>665.57</td>\n",
       "      <td>665.57</td>\n",
       "      <td>348.32</td>\n",
       "      <td>174.16</td>\n",
       "      <td>321.57</td>\n",
       "      <td>96.47</td>\n",
       "      <td>405.60</td>\n",
       "      <td>81.12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>14</td>\n",
       "      <td>1014.50</td>\n",
       "      <td>1015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640.42</td>\n",
       "      <td>640.42</td>\n",
       "      <td>393.69</td>\n",
       "      <td>196.84</td>\n",
       "      <td>350.25</td>\n",
       "      <td>105.08</td>\n",
       "      <td>360.82</td>\n",
       "      <td>72.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32</td>\n",
       "      <td>827.97</td>\n",
       "      <td>824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>326.38</td>\n",
       "      <td>326.38</td>\n",
       "      <td>665.57</td>\n",
       "      <td>332.78</td>\n",
       "      <td>348.32</td>\n",
       "      <td>104.50</td>\n",
       "      <td>321.57</td>\n",
       "      <td>64.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>28</td>\n",
       "      <td>824.50</td>\n",
       "      <td>815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>316.14</td>\n",
       "      <td>316.14</td>\n",
       "      <td>640.42</td>\n",
       "      <td>320.21</td>\n",
       "      <td>393.69</td>\n",
       "      <td>118.11</td>\n",
       "      <td>350.25</td>\n",
       "      <td>70.05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>29</td>\n",
       "      <td>807.02</td>\n",
       "      <td>823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317.94</td>\n",
       "      <td>317.94</td>\n",
       "      <td>444.73</td>\n",
       "      <td>222.36</td>\n",
       "      <td>662.73</td>\n",
       "      <td>198.82</td>\n",
       "      <td>339.52</td>\n",
       "      <td>67.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>26</td>\n",
       "      <td>848.13</td>\n",
       "      <td>855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>410.23</td>\n",
       "      <td>410.23</td>\n",
       "      <td>336.44</td>\n",
       "      <td>168.22</td>\n",
       "      <td>466.52</td>\n",
       "      <td>139.96</td>\n",
       "      <td>648.60</td>\n",
       "      <td>129.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>29</td>\n",
       "      <td>818.10</td>\n",
       "      <td>822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.90</td>\n",
       "      <td>406.90</td>\n",
       "      <td>360.49</td>\n",
       "      <td>180.24</td>\n",
       "      <td>305.98</td>\n",
       "      <td>91.79</td>\n",
       "      <td>695.82</td>\n",
       "      <td>139.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>30</td>\n",
       "      <td>818.17</td>\n",
       "      <td>818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.90</td>\n",
       "      <td>406.90</td>\n",
       "      <td>360.49</td>\n",
       "      <td>180.24</td>\n",
       "      <td>326.38</td>\n",
       "      <td>97.91</td>\n",
       "      <td>665.57</td>\n",
       "      <td>133.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>25</td>\n",
       "      <td>867.03</td>\n",
       "      <td>867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>488.00</td>\n",
       "      <td>488.00</td>\n",
       "      <td>422.15</td>\n",
       "      <td>211.07</td>\n",
       "      <td>341.68</td>\n",
       "      <td>102.50</td>\n",
       "      <td>327.31</td>\n",
       "      <td>65.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>25</td>\n",
       "      <td>875.50</td>\n",
       "      <td>871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>498.63</td>\n",
       "      <td>498.63</td>\n",
       "      <td>406.90</td>\n",
       "      <td>203.45</td>\n",
       "      <td>360.49</td>\n",
       "      <td>108.15</td>\n",
       "      <td>326.38</td>\n",
       "      <td>65.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>25</td>\n",
       "      <td>872.68</td>\n",
       "      <td>880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>473.00</td>\n",
       "      <td>473.00</td>\n",
       "      <td>461.32</td>\n",
       "      <td>230.66</td>\n",
       "      <td>349.62</td>\n",
       "      <td>104.89</td>\n",
       "      <td>320.72</td>\n",
       "      <td>64.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank  total_points  previous_points  rank_change  cur_year_avg  \\\n",
       "16    28        781.58              776          NaN        414.44   \n",
       "19    33        732.77              736          NaN        348.32   \n",
       "21    29        779.25              779          NaN        393.69   \n",
       "27    14       1018.71             1040          NaN        654.09   \n",
       "31    14       1017.32             1044          NaN        665.57   \n",
       "33    14       1014.50             1015          NaN        640.42   \n",
       "43    32        827.97              824          NaN        326.38   \n",
       "45    28        824.50              815          NaN        316.14   \n",
       "50    29        807.02              823          NaN        317.94   \n",
       "60    26        848.13              855          NaN        410.23   \n",
       "66    29        818.10              822          NaN        406.90   \n",
       "67    30        818.17              818          NaN        406.90   \n",
       "77    25        867.03              867          NaN        488.00   \n",
       "79    25        875.50              871          NaN        498.63   \n",
       "82    25        872.68              880          NaN        473.00   \n",
       "\n",
       "    cur_year_avg_weighted  last_year_avg  last_year_avg_weighted  \\\n",
       "16                 414.44         285.53                  142.76   \n",
       "19                 348.32         321.57                  160.78   \n",
       "21                 393.69         350.25                  175.13   \n",
       "27                 654.09         414.44                  207.22   \n",
       "31                 665.57         348.32                  174.16   \n",
       "33                 640.42         393.69                  196.84   \n",
       "43                 326.38         665.57                  332.78   \n",
       "45                 316.14         640.42                  320.21   \n",
       "50                 317.94         444.73                  222.36   \n",
       "60                 410.23         336.44                  168.22   \n",
       "66                 406.90         360.49                  180.24   \n",
       "67                 406.90         360.49                  180.24   \n",
       "77                 488.00         422.15                  211.07   \n",
       "79                 498.63         406.90                  203.45   \n",
       "82                 473.00         461.32                  230.66   \n",
       "\n",
       "    two_year_ago_avg  two_year_ago_avg_weighted  three_year_ago_avg  \\\n",
       "16            358.73                     107.62              583.78   \n",
       "19            405.60                     121.68              509.94   \n",
       "21            360.82                     108.24              510.95   \n",
       "27            285.53                      85.66              358.73   \n",
       "31            321.57                      96.47              405.60   \n",
       "33            350.25                     105.08              360.82   \n",
       "43            348.32                     104.50              321.57   \n",
       "45            393.69                     118.11              350.25   \n",
       "50            662.73                     198.82              339.52   \n",
       "60            466.52                     139.96              648.60   \n",
       "66            305.98                      91.79              695.82   \n",
       "67            326.38                      97.91              665.57   \n",
       "77            341.68                     102.50              327.31   \n",
       "79            360.49                     108.15              326.38   \n",
       "82            349.62                     104.89              320.72   \n",
       "\n",
       "    three_year_ago_avg_weighted  country_full_class  country_abrv_class  \\\n",
       "16                       116.76                   0                   0   \n",
       "19                       101.99                   0                   0   \n",
       "21                       102.19                   0                   0   \n",
       "27                        71.75                   0                   0   \n",
       "31                        81.12                   0                   0   \n",
       "33                        72.16                   0                   0   \n",
       "43                        64.31                   0                   0   \n",
       "45                        70.05                   0                   0   \n",
       "50                        67.90                   0                   0   \n",
       "60                       129.72                   0                   0   \n",
       "66                       139.16                   0                   0   \n",
       "67                       133.11                   0                   0   \n",
       "77                        65.46                   0                   0   \n",
       "79                        65.28                   0                   0   \n",
       "82                        64.14                   0                   0   \n",
       "\n",
       "    confederation_class  rank_date_class  \n",
       "16                    0               16  \n",
       "19                    0               19  \n",
       "21                    0               21  \n",
       "27                    0               27  \n",
       "31                    0               31  \n",
       "33                    0               33  \n",
       "43                    0               43  \n",
       "45                    0               45  \n",
       "50                    0               50  \n",
       "60                    0               60  \n",
       "66                    0               66  \n",
       "67                    0               67  \n",
       "77                    0               77  \n",
       "79                    0               79  \n",
       "82                    0               82  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_rows = df[df.isnull().T.any().T]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notnull(df['rank_change'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 16)\n",
      "(25, 16)\n",
      "(58,)\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.reshape(58,1)\n",
    "y_test = y_test.values.reshape(25,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = int(len(df)*.7)\n",
    "pt_to_split = int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 39 observations.\n",
      "Test set has 29 observations.\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df['rank_date_class']<pt_to_split]\n",
    "df_test = df[df['rank_date_class']>=pt_to_split]\n",
    "\n",
    "spots_train = df_train['total_points'].tolist()\n",
    "spots_test = df_test['total_points'].tolist()\n",
    "\n",
    "print(\"Training set has {} observations.\".format(len(spots_train)))\n",
    "print(\"Test set has {} observations.\".format(len(spots_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (33, 5, 1)\n",
      "Shape of test set: (23, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_sequences(seq_size, obs):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(obs)-SEQUENCE_SIZE-1):\n",
    "        #print(i)\n",
    "        window = obs[i: (i+SEQUENCE_SIZE)]\n",
    "        after_window = obs[i+SEQUENCE_SIZE]\n",
    "        window = [[x] for x in window]\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "SEQUENCE_SIZE = 5\n",
    "\n",
    "x_train,y_train = to_sequences(SEQUENCE_SIZE,spots_train)\n",
    "x_test,y_test = to_sequences(SEQUENCE_SIZE,spots_test)\n",
    "\n",
    "print(\"Shape of training set: {}\".format(x_train.shape))\n",
    "print(\"Shape of test set: {}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                16896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 19,009\n",
      "Trainable params: 19,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33 samples, validate on 23 samples\n",
      "Epoch 1/1000\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 739200.8371 - val_loss: 696972.8125\n",
      "Epoch 2/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 738656.5739 - val_loss: 696536.1250\n",
      "Epoch 3/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 738210.2595 - val_loss: 696102.3125\n",
      "Epoch 4/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 737768.2879 - val_loss: 695677.1250\n",
      "Epoch 5/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 737332.9830 - val_loss: 695249.5000\n",
      "Epoch 6/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 736897.5606 - val_loss: 694820.8125\n",
      "Epoch 7/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 736458.2538 - val_loss: 694390.2500\n",
      "Epoch 8/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 736020.1989 - val_loss: 693958.4375\n",
      "Epoch 9/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 735577.2879 - val_loss: 693524.0000\n",
      "Epoch 10/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 735136.5881 - val_loss: 693089.3125\n",
      "Epoch 11/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 734691.6023 - val_loss: 692655.5000\n",
      "Epoch 12/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 734247.0455 - val_loss: 692213.5000\n",
      "Epoch 13/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 733797.1667 - val_loss: 691768.3750\n",
      "Epoch 14/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 733341.6004 - val_loss: 691318.3750\n",
      "Epoch 15/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 732883.7045 - val_loss: 690864.8750\n",
      "Epoch 16/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 732419.1951 - val_loss: 690407.0000\n",
      "Epoch 17/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 731951.1383 - val_loss: 689940.3750\n",
      "Epoch 18/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 731476.3201 - val_loss: 689468.0000\n",
      "Epoch 19/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 730992.0814 - val_loss: 688986.5000\n",
      "Epoch 20/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 730501.7424 - val_loss: 688499.7500\n",
      "Epoch 21/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 730003.2311 - val_loss: 688004.6250\n",
      "Epoch 22/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 729497.9527 - val_loss: 687499.0625\n",
      "Epoch 23/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 728982.6345 - val_loss: 686986.3750\n",
      "Epoch 24/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 728457.5189 - val_loss: 686464.8125\n",
      "Epoch 25/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 727926.4602 - val_loss: 685934.3750\n",
      "Epoch 26/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 727384.5928 - val_loss: 685397.7500\n",
      "Epoch 27/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 726836.2178 - val_loss: 684852.9375\n",
      "Epoch 28/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 726280.0568 - val_loss: 684299.2500\n",
      "Epoch 29/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 725712.9659 - val_loss: 683733.1250\n",
      "Epoch 30/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 725135.8902 - val_loss: 683153.7500\n",
      "Epoch 31/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 724545.0720 - val_loss: 682566.6875\n",
      "Epoch 32/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 723942.9091 - val_loss: 681967.1250\n",
      "Epoch 33/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 723330.5777 - val_loss: 681349.3750\n",
      "Epoch 34/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 722701.1553 - val_loss: 680719.5625\n",
      "Epoch 35/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 722059.9252 - val_loss: 680086.5625\n",
      "Epoch 36/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 721409.6420 - val_loss: 679442.7500\n",
      "Epoch 37/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 720751.6250 - val_loss: 678778.3750\n",
      "Epoch 38/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 720076.9725 - val_loss: 678104.5000\n",
      "Epoch 39/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 719386.5625 - val_loss: 677424.2500\n",
      "Epoch 40/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 718692.7500 - val_loss: 676733.5625\n",
      "Epoch 41/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 717987.8636 - val_loss: 676036.1250\n",
      "Epoch 42/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 717271.5909 - val_loss: 675321.7500\n",
      "Epoch 43/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 716545.6553 - val_loss: 674591.8125\n",
      "Epoch 44/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 715796.1345 - val_loss: 673844.1875\n",
      "Epoch 45/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 715034.8750 - val_loss: 673080.5000\n",
      "Epoch 46/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 714251.7083 - val_loss: 672298.1875\n",
      "Epoch 47/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 713456.2519 - val_loss: 671500.9375\n",
      "Epoch 48/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 712641.7973 - val_loss: 670698.0625\n",
      "Epoch 49/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 711818.3655 - val_loss: 669878.8125\n",
      "Epoch 50/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 710981.3314 - val_loss: 669035.8125\n",
      "Epoch 51/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 710119.5322 - val_loss: 668170.0625\n",
      "Epoch 52/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 709234.3333 - val_loss: 667279.1250\n",
      "Epoch 53/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 708329.3911 - val_loss: 666381.4375\n",
      "Epoch 54/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 707412.5568 - val_loss: 665486.0625\n",
      "Epoch 55/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 706497.1193 - val_loss: 664587.5625\n",
      "Epoch 56/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 705574.8258 - val_loss: 663669.8125\n",
      "Epoch 57/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 704641.3655 - val_loss: 662734.5000\n",
      "Epoch 58/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 703681.2027 - val_loss: 661781.3125\n",
      "Epoch 59/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 702711.8182 - val_loss: 660811.3750\n",
      "Epoch 60/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 701719.3523 - val_loss: 659832.1875\n",
      "Epoch 61/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 700716.1515 - val_loss: 658831.3750\n",
      "Epoch 62/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 699696.7652 - val_loss: 657812.6250\n",
      "Epoch 63/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 698655.8447 - val_loss: 656782.3750\n",
      "Epoch 64/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 697604.5758 - val_loss: 655746.8125\n",
      "Epoch 65/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 696545.1799 - val_loss: 654702.1250\n",
      "Epoch 66/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 695479.8807 - val_loss: 653649.6250\n",
      "Epoch 67/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 694403.7348 - val_loss: 652590.6875\n",
      "Epoch 68/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 693323.5729 - val_loss: 651526.8125\n",
      "Epoch 69/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 692236.3078 - val_loss: 650458.2500\n",
      "Epoch 70/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 691141.8523 - val_loss: 649377.5000\n",
      "Epoch 71/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 690033.1951 - val_loss: 648263.4375\n",
      "Epoch 72/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 688900.6420 - val_loss: 647126.1875\n",
      "Epoch 73/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 687732.6420 - val_loss: 645967.2500\n",
      "Epoch 74/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 686553.2424 - val_loss: 644786.6250\n",
      "Epoch 75/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 685341.3409 - val_loss: 643584.2500\n",
      "Epoch 76/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 684117.5455 - val_loss: 642361.8750\n",
      "Epoch 77/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 682863.0227 - val_loss: 641119.7500\n",
      "Epoch 78/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 681593.1155 - val_loss: 639844.5000\n",
      "Epoch 79/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 680295.2424 - val_loss: 638554.6250\n",
      "Epoch 80/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 678977.2898 - val_loss: 637264.1875\n",
      "Epoch 81/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 677658.7689 - val_loss: 635971.1875\n",
      "Epoch 82/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 676333.3769 - val_loss: 634662.5000\n",
      "Epoch 83/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 674994.9261 - val_loss: 633326.0625\n",
      "Epoch 84/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 673633.6098 - val_loss: 631975.6250\n",
      "Epoch 85/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 672253.2008 - val_loss: 630623.0625\n",
      "Epoch 86/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 670866.6326 - val_loss: 629253.3750\n",
      "Epoch 87/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 669471.3333 - val_loss: 627869.5625\n",
      "Epoch 88/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 668049.2462 - val_loss: 626462.8750\n",
      "Epoch 89/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 666616.7405 - val_loss: 625030.8125\n",
      "Epoch 90/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 665146.9527 - val_loss: 623574.6250\n",
      "Epoch 91/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 663663.5379 - val_loss: 622096.2500\n",
      "Epoch 92/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 662154.0152 - val_loss: 620613.9375\n",
      "Epoch 93/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 660632.1439 - val_loss: 619114.4375\n",
      "Epoch 94/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 659100.3883 - val_loss: 617584.1250\n",
      "Epoch 95/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 657533.8485 - val_loss: 616025.5000\n",
      "Epoch 96/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 655947.2633 - val_loss: 614456.1250\n",
      "Epoch 97/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 654342.9451 - val_loss: 612893.1875\n",
      "Epoch 98/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 652737.4962 - val_loss: 611311.1250\n",
      "Epoch 99/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 651125.4489 - val_loss: 609707.6250\n",
      "Epoch 100/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 649481.0625 - val_loss: 608085.9375\n",
      "Epoch 101/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 647824.3674 - val_loss: 606439.4375\n",
      "Epoch 102/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 646142.9792 - val_loss: 604779.6250\n",
      "Epoch 103/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 644451.2585 - val_loss: 603130.5625\n",
      "Epoch 104/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 642762.6989 - val_loss: 601490.1875\n",
      "Epoch 105/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 641085.4081 - val_loss: 599848.6250\n",
      "Epoch 106/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 639399.0303 - val_loss: 598183.6875\n",
      "Epoch 107/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 637698.4621 - val_loss: 596483.0625\n",
      "Epoch 108/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 635959.0341 - val_loss: 594758.1250\n",
      "Epoch 109/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 634194.5492 - val_loss: 593010.3750\n",
      "Epoch 110/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 632409.4110 - val_loss: 591249.0625\n",
      "Epoch 111/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 630611.8258 - val_loss: 589491.5625\n",
      "Epoch 112/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 628813.8419 - val_loss: 587742.3750\n",
      "Epoch 113/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 627026.5653 - val_loss: 586000.6250\n",
      "Epoch 114/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 625241.6780 - val_loss: 584256.8750\n",
      "Epoch 115/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 623459.8939 - val_loss: 582504.9375\n",
      "Epoch 116/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 621659.0814 - val_loss: 580724.8750\n",
      "Epoch 117/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 619847.9564 - val_loss: 578926.0625\n",
      "Epoch 118/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 618002.9413 - val_loss: 577123.5000\n",
      "Epoch 119/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 616162.6657 - val_loss: 575313.2500\n",
      "Epoch 120/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 614312.7008 - val_loss: 573511.0000\n",
      "Epoch 121/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 612456.8163 - val_loss: 571682.8750\n",
      "Epoch 122/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 610595.9669 - val_loss: 569827.8125\n",
      "Epoch 123/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 608688.1780 - val_loss: 567949.1250\n",
      "Epoch 124/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 606774.4081 - val_loss: 566049.0625\n",
      "Epoch 125/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 604830.2263 - val_loss: 564153.5625\n",
      "Epoch 126/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 602891.7244 - val_loss: 562262.3750\n",
      "Epoch 127/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 600955.4782 - val_loss: 560372.0625\n",
      "Epoch 128/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 599019.5966 - val_loss: 558473.3750\n",
      "Epoch 129/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 597068.4280 - val_loss: 556538.9375\n",
      "Epoch 130/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 595095.8674 - val_loss: 554575.5000\n",
      "Epoch 131/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 593085.4356 - val_loss: 552605.6250\n",
      "Epoch 132/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 591074.7879 - val_loss: 550642.6250\n",
      "Epoch 133/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 589062.6032 - val_loss: 548688.3750\n",
      "Epoch 134/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 587061.4006 - val_loss: 546727.9375\n",
      "Epoch 135/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 585048.0644 - val_loss: 544739.4375\n",
      "Epoch 136/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 583016.8902 - val_loss: 542721.6250\n",
      "Epoch 137/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 580956.8665 - val_loss: 540711.7500\n",
      "Epoch 138/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 578886.1723 - val_loss: 538682.3750\n",
      "Epoch 139/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 576811.0909 - val_loss: 536609.5000\n",
      "Epoch 140/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 574699.6307 - val_loss: 534541.1875\n",
      "Epoch 141/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 572579.6004 - val_loss: 532497.4375\n",
      "Epoch 142/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 570481.8295 - val_loss: 530448.6875\n",
      "Epoch 143/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 568384.7822 - val_loss: 528382.5000\n",
      "Epoch 144/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 566269.2633 - val_loss: 526305.1250\n",
      "Epoch 145/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 564144.9337 - val_loss: 524227.7812\n",
      "Epoch 146/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 562016.4233 - val_loss: 522154.1250\n",
      "Epoch 147/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 559896.7235 - val_loss: 520090.2500\n",
      "Epoch 148/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 557767.6629 - val_loss: 517998.0938\n",
      "Epoch 149/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 555635.2102 - val_loss: 515872.4375\n",
      "Epoch 150/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 553457.8807 - val_loss: 513750.4688\n",
      "Epoch 151/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 551284.3371 - val_loss: 511629.7812\n",
      "Epoch 152/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 549113.1288 - val_loss: 509512.1875\n",
      "Epoch 153/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 546943.7633 - val_loss: 507399.5938\n",
      "Epoch 154/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 544778.1098 - val_loss: 505285.4688\n",
      "Epoch 155/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 542604.8788 - val_loss: 503143.4688\n",
      "Epoch 156/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 540420.5464 - val_loss: 500991.0000\n",
      "Epoch 157/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 538199.2879 - val_loss: 498812.4375\n",
      "Epoch 158/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 535978.3873 - val_loss: 496604.1875\n",
      "Epoch 159/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 533703.9186 - val_loss: 494372.2500\n",
      "Epoch 160/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 531419.6004 - val_loss: 492099.3125\n",
      "Epoch 161/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 529100.2973 - val_loss: 489832.9688\n",
      "Epoch 162/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 526772.1212 - val_loss: 487582.9688\n",
      "Epoch 163/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 524466.9564 - val_loss: 485332.6562\n",
      "Epoch 164/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 522149.4830 - val_loss: 483052.8750\n",
      "Epoch 165/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 519820.0170 - val_loss: 480743.3438\n",
      "Epoch 166/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 517456.5492 - val_loss: 478443.6562\n",
      "Epoch 167/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 515091.2576 - val_loss: 476140.2500\n",
      "Epoch 168/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 512725.0682 - val_loss: 473800.5938\n",
      "Epoch 169/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 510326.2330 - val_loss: 471423.0938\n",
      "Epoch 170/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 507886.7481 - val_loss: 469014.4688\n",
      "Epoch 171/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 505420.2500 - val_loss: 466588.5625\n",
      "Epoch 172/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 502942.3030 - val_loss: 464185.4062\n",
      "Epoch 173/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 500476.6610 - val_loss: 461821.6562\n",
      "Epoch 174/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 498055.6127 - val_loss: 459493.2500\n",
      "Epoch 175/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 495664.7888 - val_loss: 457192.2500\n",
      "Epoch 176/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 493303.5682 - val_loss: 454901.4062\n",
      "Epoch 177/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 490952.6241 - val_loss: 452612.3125\n",
      "Epoch 178/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 488603.0597 - val_loss: 450318.6875\n",
      "Epoch 179/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 486239.1894 - val_loss: 447987.7500\n",
      "Epoch 180/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 483846.1004 - val_loss: 445597.2500\n",
      "Epoch 181/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 481407.8778 - val_loss: 443202.0938\n",
      "Epoch 182/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 478946.4527 - val_loss: 440831.4375\n",
      "Epoch 183/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 476519.2339 - val_loss: 438487.2500\n",
      "Epoch 184/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 474110.6979 - val_loss: 436169.5625\n",
      "Epoch 185/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 471731.8485 - val_loss: 433863.6562\n",
      "Epoch 186/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 469352.3239 - val_loss: 431529.6562\n",
      "Epoch 187/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 466956.3352 - val_loss: 429141.4688\n",
      "Epoch 188/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 464514.5938 - val_loss: 426743.1875\n",
      "Epoch 189/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 462058.8205 - val_loss: 424383.4375\n",
      "Epoch 190/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 459631.4356 - val_loss: 422056.7500\n",
      "Epoch 191/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 457237.9138 - val_loss: 419728.9062\n",
      "Epoch 192/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 454856.1098 - val_loss: 417413.8750\n",
      "Epoch 193/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 452471.5104 - val_loss: 415111.5312\n",
      "Epoch 194/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 450109.8523 - val_loss: 412807.3125\n",
      "Epoch 195/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 447739.9252 - val_loss: 410500.7812\n",
      "Epoch 196/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 445373.4754 - val_loss: 408191.0312\n",
      "Epoch 197/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 442986.2102 - val_loss: 405845.7500\n",
      "Epoch 198/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 440593.5227 - val_loss: 403485.1250\n",
      "Epoch 199/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 438150.8977 - val_loss: 401110.0000\n",
      "Epoch 200/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 435725.3977 - val_loss: 398720.5625\n",
      "Epoch 201/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 433269.1686 - val_loss: 396359.2500\n",
      "Epoch 202/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 430843.9006 - val_loss: 394022.9062\n",
      "Epoch 203/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 428441.2538 - val_loss: 391702.7500\n",
      "Epoch 204/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 426044.4356 - val_loss: 389357.3438\n",
      "Epoch 205/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 423645.4848 - val_loss: 386994.9062\n",
      "Epoch 206/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 421216.5758 - val_loss: 384652.2188\n",
      "Epoch 207/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 418807.9858 - val_loss: 382325.5625\n",
      "Epoch 208/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 416420.5388 - val_loss: 380024.5312\n",
      "Epoch 209/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 414053.3598 - val_loss: 377753.6875\n",
      "Epoch 210/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 411706.2898 - val_loss: 375466.5625\n",
      "Epoch 211/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 409351.8087 - val_loss: 373127.4375\n",
      "Epoch 212/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 406940.2727 - val_loss: 370730.3125\n",
      "Epoch 213/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 404486.1364 - val_loss: 368310.6562\n",
      "Epoch 214/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 402002.4673 - val_loss: 365923.2500\n",
      "Epoch 215/1000\n",
      "33/33 [==============================] - ETA: 0s - loss: 392590.87 - 0s 2ms/step - loss: 399530.0095 - val_loss: 363528.1562\n",
      "Epoch 216/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 397077.7888 - val_loss: 361119.1875\n",
      "Epoch 217/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 394606.0185 - val_loss: 358754.5625\n",
      "Epoch 218/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 392157.0445 - val_loss: 356397.5625\n",
      "Epoch 219/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 389741.0824 - val_loss: 354031.7500\n",
      "Epoch 220/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 387290.7898 - val_loss: 351645.9688\n",
      "Epoch 221/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 384853.0753 - val_loss: 349256.7500\n",
      "Epoch 222/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 382387.0464 - val_loss: 346897.8125\n",
      "Epoch 223/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 379952.2263 - val_loss: 344527.1562\n",
      "Epoch 224/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 377515.1061 - val_loss: 342144.2500\n",
      "Epoch 225/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 375071.3054 - val_loss: 339793.2500\n",
      "Epoch 226/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 372643.4972 - val_loss: 337475.6875\n",
      "Epoch 227/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 370244.3182 - val_loss: 335130.5312\n",
      "Epoch 228/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 367846.9972 - val_loss: 332786.6875\n",
      "Epoch 229/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 365424.5208 - val_loss: 330473.4375\n",
      "Epoch 230/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 363044.3589 - val_loss: 328171.8125\n",
      "Epoch 231/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 360679.5289 - val_loss: 325909.1875\n",
      "Epoch 232/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 358341.8958 - val_loss: 323677.8438\n",
      "Epoch 233/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 356026.8144 - val_loss: 321407.5625\n",
      "Epoch 234/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 353702.7884 - val_loss: 319114.5312\n",
      "Epoch 235/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 351341.2150 - val_loss: 316858.9062\n",
      "Epoch 236/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 349011.3864 - val_loss: 314624.8750\n",
      "Epoch 237/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 346712.6070 - val_loss: 312409.7500\n",
      "Epoch 238/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 344415.7509 - val_loss: 310184.6562\n",
      "Epoch 239/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 342116.8305 - val_loss: 307905.7500\n",
      "Epoch 240/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 339774.7936 - val_loss: 305603.7500\n",
      "Epoch 241/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 337405.6250 - val_loss: 303326.0938\n",
      "Epoch 242/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 335039.8939 - val_loss: 301035.7500\n",
      "Epoch 243/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 332690.9583 - val_loss: 298734.6875\n",
      "Epoch 244/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 330306.4877 - val_loss: 296438.1875\n",
      "Epoch 245/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 327935.7973 - val_loss: 294116.6875\n",
      "Epoch 246/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 325546.5871 - val_loss: 291797.9375\n",
      "Epoch 247/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 323140.6932 - val_loss: 289466.5312\n",
      "Epoch 248/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 320751.4190 - val_loss: 287145.2188\n",
      "Epoch 249/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 318352.4749 - val_loss: 284874.0938\n",
      "Epoch 250/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 316014.3949 - val_loss: 282656.8750\n",
      "Epoch 251/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 313703.5388 - val_loss: 280439.0312\n",
      "Epoch 252/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 311426.4981 - val_loss: 278204.8125\n",
      "Epoch 253/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 309103.6619 - val_loss: 275956.4688\n",
      "Epoch 254/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 306795.9640 - val_loss: 273703.1875\n",
      "Epoch 255/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 304464.9536 - val_loss: 271482.4688\n",
      "Epoch 256/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 302177.6610 - val_loss: 269302.7188\n",
      "Epoch 257/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 299908.8087 - val_loss: 267129.1562\n",
      "Epoch 258/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 297656.1619 - val_loss: 264901.8438\n",
      "Epoch 259/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 295358.5047 - val_loss: 262637.8125\n",
      "Epoch 260/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 293028.4967 - val_loss: 260394.4844\n",
      "Epoch 261/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 290704.5516 - val_loss: 258186.6562\n",
      "Epoch 262/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 288425.1473 - val_loss: 256007.1719\n",
      "Epoch 263/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 286170.7704 - val_loss: 253863.2812\n",
      "Epoch 264/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 283937.6610 - val_loss: 251703.7031\n",
      "Epoch 265/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 281718.3130 - val_loss: 249537.4531\n",
      "Epoch 266/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 279472.4493 - val_loss: 247399.2969\n",
      "Epoch 267/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 277269.1023 - val_loss: 245300.9531\n",
      "Epoch 268/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 275090.4446 - val_loss: 243241.7969\n",
      "Epoch 269/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 272949.8286 - val_loss: 241169.4531\n",
      "Epoch 270/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 270817.1075 - val_loss: 239103.8750\n",
      "Epoch 271/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 268659.4991 - val_loss: 237028.3750\n",
      "Epoch 272/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 266524.3310 - val_loss: 234939.2969\n",
      "Epoch 273/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 264343.5616 - val_loss: 232835.9062\n",
      "Epoch 274/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 262175.9806 - val_loss: 230713.0938\n",
      "Epoch 275/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 259976.2959 - val_loss: 228611.7031\n",
      "Epoch 276/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 257805.1027 - val_loss: 226548.2656\n",
      "Epoch 277/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 255663.8272 - val_loss: 224526.8438\n",
      "Epoch 278/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 253552.9574 - val_loss: 222487.7188\n",
      "Epoch 279/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 251452.9863 - val_loss: 220438.5469\n",
      "Epoch 280/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 249317.8248 - val_loss: 218396.3750\n",
      "Epoch 281/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 247216.4699 - val_loss: 216383.2969\n",
      "Epoch 282/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 245108.2481 - val_loss: 214383.1250\n",
      "Epoch 283/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 243029.9403 - val_loss: 212333.2969\n",
      "Epoch 284/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 240923.6977 - val_loss: 210302.3906\n",
      "Epoch 285/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 238804.8258 - val_loss: 208315.2188\n",
      "Epoch 286/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 236732.8845 - val_loss: 206307.7031\n",
      "Epoch 287/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 234665.8731 - val_loss: 204313.9844\n",
      "Epoch 288/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 232592.2902 - val_loss: 202372.1719\n",
      "Epoch 289/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 230558.8116 - val_loss: 200415.0625\n",
      "Epoch 290/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 228538.7292 - val_loss: 198442.6250\n",
      "Epoch 291/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 226474.8191 - val_loss: 196457.9844\n",
      "Epoch 292/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 224434.6626 - val_loss: 194491.8906\n",
      "Epoch 293/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 222371.6733 - val_loss: 192548.2031\n",
      "Epoch 294/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 220346.7027 - val_loss: 190561.7969\n",
      "Epoch 295/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 218279.2206 - val_loss: 188533.7656\n",
      "Epoch 296/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 216188.3887 - val_loss: 186524.8906\n",
      "Epoch 297/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 214080.7642 - val_loss: 184532.8906\n",
      "Epoch 298/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 212006.6013 - val_loss: 182508.2656\n",
      "Epoch 299/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 209898.9006 - val_loss: 180460.0156\n",
      "Epoch 300/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 207768.8447 - val_loss: 178403.0469\n",
      "Epoch 301/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 205623.5947 - val_loss: 176341.5625\n",
      "Epoch 302/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 203492.6927 - val_loss: 174312.2188\n",
      "Epoch 303/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 201389.8345 - val_loss: 172377.6562\n",
      "Epoch 304/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 199367.6527 - val_loss: 170526.9062\n",
      "Epoch 305/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 197439.5225 - val_loss: 168726.8750\n",
      "Epoch 306/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 195546.2500 - val_loss: 166920.5938\n",
      "Epoch 307/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 193661.8485 - val_loss: 165066.9219\n",
      "Epoch 308/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 191752.7848 - val_loss: 163245.3750\n",
      "Epoch 309/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 189842.5554 - val_loss: 161485.4844\n",
      "Epoch 310/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 188007.2765 - val_loss: 159750.0938\n",
      "Epoch 311/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 186202.1764 - val_loss: 158054.9531\n",
      "Epoch 312/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 184413.8485 - val_loss: 156353.5156\n",
      "Epoch 313/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 182635.7661 - val_loss: 154595.8750\n",
      "Epoch 314/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 180798.6089 - val_loss: 152794.9062\n",
      "Epoch 315/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 178938.9325 - val_loss: 151020.4844\n",
      "Epoch 316/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 177065.9129 - val_loss: 149272.1562\n",
      "Epoch 317/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 175260.3832 - val_loss: 147556.0156\n",
      "Epoch 318/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 173464.8265 - val_loss: 145919.0000\n",
      "Epoch 319/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 171749.9119 - val_loss: 144330.2812\n",
      "Epoch 320/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 170083.3490 - val_loss: 142757.5625\n",
      "Epoch 321/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 168426.3854 - val_loss: 141151.5469\n",
      "Epoch 322/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 166763.5246 - val_loss: 139542.8125\n",
      "Epoch 323/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 165083.5910 - val_loss: 137995.7344\n",
      "Epoch 324/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 163461.9879 - val_loss: 136501.8906\n",
      "Epoch 325/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 161890.2931 - val_loss: 135022.7969\n",
      "Epoch 326/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 160330.1061 - val_loss: 133504.1094\n",
      "Epoch 327/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 158755.4624 - val_loss: 131977.7969\n",
      "Epoch 328/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 157149.3011 - val_loss: 130476.2969\n",
      "Epoch 329/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 155570.4796 - val_loss: 128963.9922\n",
      "Epoch 330/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 153994.0455 - val_loss: 127460.4141\n",
      "Epoch 331/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 152413.9148 - val_loss: 125982.7266\n",
      "Epoch 332/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 150867.4200 - val_loss: 124529.3906\n",
      "Epoch 333/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 149332.5687 - val_loss: 123078.9141\n",
      "Epoch 334/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 147825.1075 - val_loss: 121650.3984\n",
      "Epoch 335/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 146310.4067 - val_loss: 120239.4141\n",
      "Epoch 336/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 144837.4924 - val_loss: 118829.8594\n",
      "Epoch 337/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 143360.4485 - val_loss: 117461.1406\n",
      "Epoch 338/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 141915.5331 - val_loss: 116116.3516\n",
      "Epoch 339/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 140502.4273 - val_loss: 114775.1172\n",
      "Epoch 340/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 139098.3326 - val_loss: 113459.4531\n",
      "Epoch 341/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 137712.0426 - val_loss: 112175.5078\n",
      "Epoch 342/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 136357.3075 - val_loss: 110898.5000\n",
      "Epoch 343/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 135018.1200 - val_loss: 109632.4922\n",
      "Epoch 344/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 133670.3348 - val_loss: 108344.7812\n",
      "Epoch 345/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 132312.3679 - val_loss: 106996.5469\n",
      "Epoch 346/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 130896.0052 - val_loss: 105616.0234\n",
      "Epoch 347/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 129435.5994 - val_loss: 104211.4375\n",
      "Epoch 348/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 127977.1302 - val_loss: 102837.2578\n",
      "Epoch 349/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 126522.0076 - val_loss: 101528.6484\n",
      "Epoch 350/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 125130.9375 - val_loss: 100225.2031\n",
      "Epoch 351/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 123762.6742 - val_loss: 98919.5312\n",
      "Epoch 352/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 122385.9744 - val_loss: 97638.5547\n",
      "Epoch 353/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 121035.7150 - val_loss: 96388.7031\n",
      "Epoch 354/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 119722.4765 - val_loss: 95190.5312\n",
      "Epoch 355/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 118441.6056 - val_loss: 94007.9453\n",
      "Epoch 356/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 117202.7797 - val_loss: 92830.7578\n",
      "Epoch 357/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 115940.6364 - val_loss: 91639.8672\n",
      "Epoch 358/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 114697.1478 - val_loss: 90440.3906\n",
      "Epoch 359/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 113421.7585 - val_loss: 89264.8359\n",
      "Epoch 360/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 112184.8755 - val_loss: 88113.8047\n",
      "Epoch 361/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 110960.6323 - val_loss: 86987.8984\n",
      "Epoch 362/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 109774.7038 - val_loss: 85887.5000\n",
      "Epoch 363/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 108607.2496 - val_loss: 84823.1016\n",
      "Epoch 364/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 107476.5330 - val_loss: 83776.2109\n",
      "Epoch 365/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 106354.5114 - val_loss: 82700.8359\n",
      "Epoch 366/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 105211.8447 - val_loss: 81569.4219\n",
      "Epoch 367/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 104021.0135 - val_loss: 80425.3516\n",
      "Epoch 368/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 102808.8950 - val_loss: 79308.8281\n",
      "Epoch 369/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 101623.3488 - val_loss: 78222.2422\n",
      "Epoch 370/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 100470.2182 - val_loss: 77165.0781\n",
      "Epoch 371/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 99333.0563 - val_loss: 76096.6953\n",
      "Epoch 372/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 98204.7438 - val_loss: 75009.0625\n",
      "Epoch 373/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 97039.8665 - val_loss: 73910.8594\n",
      "Epoch 374/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 95880.0102 - val_loss: 72812.2422\n",
      "Epoch 375/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 94699.5866 - val_loss: 71711.4688\n",
      "Epoch 376/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 93540.7829 - val_loss: 70621.7812\n",
      "Epoch 377/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 92379.8820 - val_loss: 69579.4375\n",
      "Epoch 378/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 91268.7901 - val_loss: 68577.4609\n",
      "Epoch 379/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 90198.0626 - val_loss: 67603.4922\n",
      "Epoch 380/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 89160.7185 - val_loss: 66654.1094\n",
      "Epoch 381/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 88149.6177 - val_loss: 65738.7109\n",
      "Epoch 382/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 87166.6025 - val_loss: 64842.9180\n",
      "Epoch 383/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 86212.0265 - val_loss: 63957.8906\n",
      "Epoch 384/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 85266.1366 - val_loss: 63092.4297\n",
      "Epoch 385/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 84326.9678 - val_loss: 62204.2266\n",
      "Epoch 386/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 83388.9194 - val_loss: 61301.2539\n",
      "Epoch 387/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 82425.4764 - val_loss: 60435.8047\n",
      "Epoch 388/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 81498.6777 - val_loss: 59611.0391\n",
      "Epoch 389/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 80598.5436 - val_loss: 58769.9727\n",
      "Epoch 390/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 79709.2307 - val_loss: 57910.8633\n",
      "Epoch 391/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 78775.1330 - val_loss: 57041.8711\n",
      "Epoch 392/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 77840.7609 - val_loss: 56131.7461\n",
      "Epoch 393/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 76874.2754 - val_loss: 55221.5938\n",
      "Epoch 394/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 75896.5694 - val_loss: 54343.0586\n",
      "Epoch 395/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 74954.2810 - val_loss: 53497.2070\n",
      "Epoch 396/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 74041.5109 - val_loss: 52675.8789\n",
      "Epoch 397/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 73156.0814 - val_loss: 51863.8750\n",
      "Epoch 398/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 72273.0024 - val_loss: 51033.9453\n",
      "Epoch 399/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 71387.8381 - val_loss: 50196.7461\n",
      "Epoch 400/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 70475.0521 - val_loss: 49354.6367\n",
      "Epoch 401/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 69582.8556 - val_loss: 48528.9609\n",
      "Epoch 402/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 68677.0185 - val_loss: 47720.7188\n",
      "Epoch 403/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 67815.0133 - val_loss: 46921.7070\n",
      "Epoch 404/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 66948.4691 - val_loss: 46152.7461\n",
      "Epoch 405/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 66123.0183 - val_loss: 45418.4258\n",
      "Epoch 406/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 65330.9258 - val_loss: 44736.4023\n",
      "Epoch 407/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 64576.0824 - val_loss: 44054.9922\n",
      "Epoch 408/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 63852.9289 - val_loss: 43375.8359\n",
      "Epoch 409/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 63112.1734 - val_loss: 42731.5625\n",
      "Epoch 410/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 62415.9443 - val_loss: 42112.3398\n",
      "Epoch 411/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 61734.0289 - val_loss: 41491.8242\n",
      "Epoch 412/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 61053.9257 - val_loss: 40827.6562\n",
      "Epoch 413/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 60335.7157 - val_loss: 40131.0781\n",
      "Epoch 414/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 59586.6565 - val_loss: 39448.4258\n",
      "Epoch 415/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 58843.5756 - val_loss: 38800.4375\n",
      "Epoch 416/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 58137.6960 - val_loss: 38180.3867\n",
      "Epoch 417/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 57460.8694 - val_loss: 37580.4375\n",
      "Epoch 418/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 56808.7529 - val_loss: 37001.6367\n",
      "Epoch 419/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 56179.9748 - val_loss: 36455.8984\n",
      "Epoch 420/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 55577.5723 - val_loss: 35925.9961\n",
      "Epoch 421/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 55000.7879 - val_loss: 35398.7188\n",
      "Epoch 422/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 54426.4951 - val_loss: 34889.9414\n",
      "Epoch 423/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 53859.9209 - val_loss: 34372.1602\n",
      "Epoch 424/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 53289.1061 - val_loss: 33810.2461\n",
      "Epoch 425/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 52674.6300 - val_loss: 33215.2383\n",
      "Epoch 426/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 52026.1735 - val_loss: 32615.3906\n",
      "Epoch 427/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 51376.7940 - val_loss: 32048.6172\n",
      "Epoch 428/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 50754.2245 - val_loss: 31531.8340\n",
      "Epoch 429/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 50183.9824 - val_loss: 31049.7637\n",
      "Epoch 430/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 49648.3177 - val_loss: 30573.7344\n",
      "Epoch 431/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 49120.8471 - val_loss: 30078.7363\n",
      "Epoch 432/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 48579.7101 - val_loss: 29575.6133\n",
      "Epoch 433/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 48024.3762 - val_loss: 29079.5117\n",
      "Epoch 434/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 47478.4746 - val_loss: 28593.6816\n",
      "Epoch 435/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 46940.7633 - val_loss: 28118.5488\n",
      "Epoch 436/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 46417.5768 - val_loss: 27656.1582\n",
      "Epoch 437/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 45894.9593 - val_loss: 27178.1523\n",
      "Epoch 438/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 45379.8811 - val_loss: 26700.3379\n",
      "Epoch 439/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 44846.8087 - val_loss: 26254.1445\n",
      "Epoch 440/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 44349.5172 - val_loss: 25817.8242\n",
      "Epoch 441/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 43869.1863 - val_loss: 25396.0273\n",
      "Epoch 442/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 43388.2874 - val_loss: 24961.4023\n",
      "Epoch 443/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 42912.2783 - val_loss: 24507.5059\n",
      "Epoch 444/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 42409.5222 - val_loss: 24070.1191\n",
      "Epoch 445/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 41919.7815 - val_loss: 23646.7715\n",
      "Epoch 446/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 41453.4069 - val_loss: 23245.6523\n",
      "Epoch 447/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 41000.1091 - val_loss: 22862.6582\n",
      "Epoch 448/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 40570.9462 - val_loss: 22476.5820\n",
      "Epoch 449/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 40142.7009 - val_loss: 22097.3457\n",
      "Epoch 450/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39707.4373 - val_loss: 21703.2617\n",
      "Epoch 451/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39273.9045 - val_loss: 21294.5156\n",
      "Epoch 452/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 38821.5218 - val_loss: 20916.4805\n",
      "Epoch 453/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 38390.4788 - val_loss: 20560.8262\n",
      "Epoch 454/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 37990.5336 - val_loss: 20206.3594\n",
      "Epoch 455/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 37592.9747 - val_loss: 19858.0977\n",
      "Epoch 456/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 37193.1562 - val_loss: 19497.9082\n",
      "Epoch 457/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 36793.2704 - val_loss: 19128.5195\n",
      "Epoch 458/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 36380.1709 - val_loss: 18782.2852\n",
      "Epoch 459/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 35983.4351 - val_loss: 18449.9297\n",
      "Epoch 460/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 35608.9614 - val_loss: 18119.9258\n",
      "Epoch 461/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 35239.5880 - val_loss: 17810.2734\n",
      "Epoch 462/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 34877.2041 - val_loss: 17495.3965\n",
      "Epoch 463/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 34525.3178 - val_loss: 17164.3496\n",
      "Epoch 464/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 34142.7687 - val_loss: 16823.1621\n",
      "Epoch 465/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 33760.1753 - val_loss: 16475.8633\n",
      "Epoch 466/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 33365.2825 - val_loss: 16142.7705\n",
      "Epoch 467/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 32983.3592 - val_loss: 15819.2773\n",
      "Epoch 468/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 32618.6755 - val_loss: 15512.7061\n",
      "Epoch 469/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 32256.5526 - val_loss: 15201.7842\n",
      "Epoch 470/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 31907.2549 - val_loss: 14879.8369\n",
      "Epoch 471/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 31529.5036 - val_loss: 14549.4941\n",
      "Epoch 472/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 31163.8608 - val_loss: 14228.3057\n",
      "Epoch 473/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 30783.6384 - val_loss: 13922.1377\n",
      "Epoch 474/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 30435.6865 - val_loss: 13617.7783\n",
      "Epoch 475/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 30077.5137 - val_loss: 13311.9697\n",
      "Epoch 476/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 29724.4953 - val_loss: 12994.1943\n",
      "Epoch 477/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 29363.8849 - val_loss: 12688.4033\n",
      "Epoch 478/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 28998.4716 - val_loss: 12385.1357\n",
      "Epoch 479/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 28654.5157 - val_loss: 12082.7773\n",
      "Epoch 480/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 28300.3286 - val_loss: 11797.2441\n",
      "Epoch 481/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 27960.8710 - val_loss: 11508.0322\n",
      "Epoch 482/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 27623.0002 - val_loss: 11206.9141\n",
      "Epoch 483/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 27281.7242 - val_loss: 10923.7139\n",
      "Epoch 484/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 26944.3787 - val_loss: 10667.9443\n",
      "Epoch 485/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 26635.5466 - val_loss: 10407.4746\n",
      "Epoch 486/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 26327.1151 - val_loss: 10127.6318\n",
      "Epoch 487/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 25997.5052 - val_loss: 9838.0762\n",
      "Epoch 488/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 25664.6407 - val_loss: 9564.9600\n",
      "Epoch 489/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 25329.8369 - val_loss: 9303.2002\n",
      "Epoch 490/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 25017.5810 - val_loss: 9032.6074\n",
      "Epoch 491/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 24706.6965 - val_loss: 8778.8789\n",
      "Epoch 492/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 24393.8403 - val_loss: 8542.7139\n",
      "Epoch 493/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 24117.5039 - val_loss: 8318.8115\n",
      "Epoch 494/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 23842.2383 - val_loss: 8106.6523\n",
      "Epoch 495/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 23592.0164 - val_loss: 7905.1470\n",
      "Epoch 496/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 23339.9986 - val_loss: 7706.2085\n",
      "Epoch 497/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 23096.8258 - val_loss: 7491.5142\n",
      "Epoch 498/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 22848.0326 - val_loss: 7287.3545\n",
      "Epoch 499/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 22588.1501 - val_loss: 7093.0845\n",
      "Epoch 500/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 22351.9059 - val_loss: 6891.5884\n",
      "Epoch 501/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 22109.3233 - val_loss: 6698.0142\n",
      "Epoch 502/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 21874.8967 - val_loss: 6526.5620\n",
      "Epoch 503/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 21661.6371 - val_loss: 6376.7061\n",
      "Epoch 504/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 21476.4180 - val_loss: 6242.3960\n",
      "Epoch 505/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 21307.0059 - val_loss: 6115.0273\n",
      "Epoch 506/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 21149.5062 - val_loss: 5990.1533\n",
      "Epoch 507/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 20993.7000 - val_loss: 5870.2676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 508/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 20839.5207 - val_loss: 5744.1997\n",
      "Epoch 509/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 20679.0239 - val_loss: 5600.6113\n",
      "Epoch 510/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 20505.9096 - val_loss: 5457.2627\n",
      "Epoch 511/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 20317.7749 - val_loss: 5315.3989\n",
      "Epoch 512/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 20145.6097 - val_loss: 5176.6812\n",
      "Epoch 513/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 19967.3365 - val_loss: 5051.4653\n",
      "Epoch 514/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 19808.0369 - val_loss: 4934.3579\n",
      "Epoch 515/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 19661.0938 - val_loss: 4830.8857\n",
      "Epoch 516/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 19526.6674 - val_loss: 4740.9185\n",
      "Epoch 517/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 19413.1614 - val_loss: 4664.8599\n",
      "Epoch 518/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 19313.6869 - val_loss: 4603.1973\n",
      "Epoch 519/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 19229.3234 - val_loss: 4533.4873\n",
      "Epoch 520/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 19137.3928 - val_loss: 4441.5552\n",
      "Epoch 521/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 19021.7357 - val_loss: 4343.5029\n",
      "Epoch 522/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 18889.6068 - val_loss: 4241.9302\n",
      "Epoch 523/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 18760.4233 - val_loss: 4137.9233\n",
      "Epoch 524/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 18625.3061 - val_loss: 4043.6304\n",
      "Epoch 525/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 18497.1044 - val_loss: 3951.9341\n",
      "Epoch 526/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 18372.6049 - val_loss: 3849.7083\n",
      "Epoch 527/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 18243.7871 - val_loss: 3748.5627\n",
      "Epoch 528/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 18109.9841 - val_loss: 3664.2324\n",
      "Epoch 529/1000\n",
      "33/33 [==============================] - 0s 990us/step - loss: 17995.2490 - val_loss: 3593.0374\n",
      "Epoch 530/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 17899.8225 - val_loss: 3531.0684\n",
      "Epoch 531/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 17814.4107 - val_loss: 3473.6548\n",
      "Epoch 532/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 17734.7091 - val_loss: 3411.2671\n",
      "Epoch 533/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 17651.8745 - val_loss: 3346.4595\n",
      "Epoch 534/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 17564.1655 - val_loss: 3287.3613\n",
      "Epoch 535/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 17483.0601 - val_loss: 3234.5957\n",
      "Epoch 536/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 17410.4392 - val_loss: 3186.9104\n",
      "Epoch 537/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 17343.2660 - val_loss: 3139.4626\n",
      "Epoch 538/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 17276.2546 - val_loss: 3085.4077\n",
      "Epoch 539/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 17199.0576 - val_loss: 3019.7375\n",
      "Epoch 540/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 17112.2106 - val_loss: 2952.5588\n",
      "Epoch 541/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 17018.1115 - val_loss: 2894.7671\n",
      "Epoch 542/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16935.3673 - val_loss: 2842.1160\n",
      "Epoch 543/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16861.9073 - val_loss: 2793.5188\n",
      "Epoch 544/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 16791.8169 - val_loss: 2748.2068\n",
      "Epoch 545/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16723.7848 - val_loss: 2695.2673\n",
      "Epoch 546/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16646.4596 - val_loss: 2629.0457\n",
      "Epoch 547/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16552.4631 - val_loss: 2556.9861\n",
      "Epoch 548/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16452.0425 - val_loss: 2490.6101\n",
      "Epoch 549/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16348.5739 - val_loss: 2424.9053\n",
      "Epoch 550/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16259.7866 - val_loss: 2362.9023\n",
      "Epoch 551/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16168.2674 - val_loss: 2316.9741\n",
      "Epoch 552/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16095.9657 - val_loss: 2279.4419\n",
      "Epoch 553/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 16036.5020 - val_loss: 2236.6201\n",
      "Epoch 554/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 15977.6510 - val_loss: 2195.4355\n",
      "Epoch 555/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 15908.9831 - val_loss: 2155.2649\n",
      "Epoch 556/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15852.4818 - val_loss: 2115.4521\n",
      "Epoch 557/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15789.7729 - val_loss: 2083.8767\n",
      "Epoch 558/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15740.5360 - val_loss: 2057.0271\n",
      "Epoch 559/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15697.4653 - val_loss: 2030.9932\n",
      "Epoch 560/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15657.4189 - val_loss: 2005.2083\n",
      "Epoch 561/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15615.9115 - val_loss: 1979.7544\n",
      "Epoch 562/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15573.3649 - val_loss: 1947.9487\n",
      "Epoch 563/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 15521.8321 - val_loss: 1907.6110\n",
      "Epoch 564/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15457.5270 - val_loss: 1863.5110\n",
      "Epoch 565/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 15385.6677 - val_loss: 1818.5964\n",
      "Epoch 566/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15315.5815 - val_loss: 1777.8264\n",
      "Epoch 567/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15245.9333 - val_loss: 1742.3585\n",
      "Epoch 568/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15189.6388 - val_loss: 1713.5074\n",
      "Epoch 569/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15137.4604 - val_loss: 1689.6033\n",
      "Epoch 570/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15097.4179 - val_loss: 1667.2483\n",
      "Epoch 571/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 15058.9311 - val_loss: 1648.2526\n",
      "Epoch 572/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 15027.0250 - val_loss: 1635.0973\n",
      "Epoch 573/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 15002.4029 - val_loss: 1626.0338\n",
      "Epoch 574/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14987.1439 - val_loss: 1620.0197\n",
      "Epoch 575/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14976.0288 - val_loss: 1618.8329\n",
      "Epoch 576/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14973.5920 - val_loss: 1612.9531\n",
      "Epoch 577/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14963.1241 - val_loss: 1600.8337\n",
      "Epoch 578/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14940.5125 - val_loss: 1585.9990\n",
      "Epoch 579/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14914.3247 - val_loss: 1568.0829\n",
      "Epoch 580/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14882.3128 - val_loss: 1550.1727\n",
      "Epoch 581/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14850.7819 - val_loss: 1534.9758\n",
      "Epoch 582/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 14822.5172 - val_loss: 1523.4215\n",
      "Epoch 583/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 14801.0494 - val_loss: 1514.4120\n",
      "Epoch 584/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14784.3194 - val_loss: 1507.8091\n",
      "Epoch 585/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 14770.8059 - val_loss: 1498.2334\n",
      "Epoch 586/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 14754.4143 - val_loss: 1486.9274\n",
      "Epoch 587/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14732.4978 - val_loss: 1478.6416\n",
      "Epoch 588/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14717.3702 - val_loss: 1474.1617\n",
      "Epoch 589/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14707.8142 - val_loss: 1470.8037\n",
      "Epoch 590/1000\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 14701.6189 - val_loss: 1466.2026\n",
      "Epoch 591/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14692.7046 - val_loss: 1461.9000\n",
      "Epoch 592/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14683.6689 - val_loss: 1453.3676\n",
      "Epoch 593/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14668.2066 - val_loss: 1442.0907\n",
      "Epoch 594/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14646.2862 - val_loss: 1433.0999\n",
      "Epoch 595/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14628.5462 - val_loss: 1426.0430\n",
      "Epoch 596/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14614.4470 - val_loss: 1420.0062\n",
      "Epoch 597/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14601.4296 - val_loss: 1410.4718\n",
      "Epoch 598/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14584.2669 - val_loss: 1399.6555\n",
      "Epoch 599/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14562.3450 - val_loss: 1392.9524\n",
      "Epoch 600/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14547.4193 - val_loss: 1385.6575\n",
      "Epoch 601/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14533.4276 - val_loss: 1377.0377\n",
      "Epoch 602/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14513.7802 - val_loss: 1365.4224\n",
      "Epoch 603/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14488.0478 - val_loss: 1347.1168\n",
      "Epoch 604/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14448.1579 - val_loss: 1324.0963\n",
      "Epoch 605/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14403.5091 - val_loss: 1302.0754\n",
      "Epoch 606/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14356.6690 - val_loss: 1286.2570\n",
      "Epoch 607/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14318.7489 - val_loss: 1275.4187\n",
      "Epoch 608/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14292.6605 - val_loss: 1266.4139\n",
      "Epoch 609/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14269.5078 - val_loss: 1255.4709\n",
      "Epoch 610/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14241.6191 - val_loss: 1240.5632\n",
      "Epoch 611/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14207.3620 - val_loss: 1225.1031\n",
      "Epoch 612/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14169.5107 - val_loss: 1212.6644\n",
      "Epoch 613/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14137.1229 - val_loss: 1204.1071\n",
      "Epoch 614/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14109.8443 - val_loss: 1195.4740\n",
      "Epoch 615/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 14084.2325 - val_loss: 1183.8511\n",
      "Epoch 616/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 14052.8037 - val_loss: 1172.2568\n",
      "Epoch 617/1000\n",
      "33/33 [==============================] - 0s 921us/step - loss: 14015.6329 - val_loss: 1161.8263\n",
      "Epoch 618/1000\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 13986.5407 - val_loss: 1153.8944\n",
      "Epoch 619/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13957.7981 - val_loss: 1149.5172\n",
      "Epoch 620/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13939.9157 - val_loss: 1146.4121\n",
      "Epoch 621/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13928.4159 - val_loss: 1143.3381\n",
      "Epoch 622/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13915.5833 - val_loss: 1139.6696\n",
      "Epoch 623/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13902.7358 - val_loss: 1136.2256\n",
      "Epoch 624/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13888.2496 - val_loss: 1134.3939\n",
      "Epoch 625/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13880.0289 - val_loss: 1134.2312\n",
      "Epoch 626/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13879.1075 - val_loss: 1133.4695\n",
      "Epoch 627/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13875.6902 - val_loss: 1131.7184\n",
      "Epoch 628/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13867.8165 - val_loss: 1130.7921\n",
      "Epoch 629/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13862.8237 - val_loss: 1129.0682\n",
      "Epoch 630/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13855.3099 - val_loss: 1126.6102\n",
      "Epoch 631/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13842.8801 - val_loss: 1124.6143\n",
      "Epoch 632/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13833.0018 - val_loss: 1123.0785\n",
      "Epoch 633/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13824.7605 - val_loss: 1122.2607\n",
      "Epoch 634/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13820.1522 - val_loss: 1122.3969\n",
      "Epoch 635/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13820.9888 - val_loss: 1122.1711\n",
      "Epoch 636/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13819.3430 - val_loss: 1121.1133\n",
      "Epoch 637/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13812.4620 - val_loss: 1119.3411\n",
      "Epoch 638/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13802.9886 - val_loss: 1117.2760\n",
      "Epoch 639/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13789.6204 - val_loss: 1115.8218\n",
      "Epoch 640/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13779.7553 - val_loss: 1114.9507\n",
      "Epoch 641/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13772.6341 - val_loss: 1114.2928\n",
      "Epoch 642/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13767.9314 - val_loss: 1113.7614\n",
      "Epoch 643/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13763.0266 - val_loss: 1113.0441\n",
      "Epoch 644/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13756.0721 - val_loss: 1111.6868\n",
      "Epoch 645/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13745.5036 - val_loss: 1110.5095\n",
      "Epoch 646/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13730.4258 - val_loss: 1109.5344\n",
      "Epoch 647/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13719.3219 - val_loss: 1108.7635\n",
      "Epoch 648/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13705.7183 - val_loss: 1108.3805\n",
      "Epoch 649/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13696.8784 - val_loss: 1108.2493\n",
      "Epoch 650/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13691.7483 - val_loss: 1108.1284\n",
      "Epoch 651/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13688.1345 - val_loss: 1108.0426\n",
      "Epoch 652/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13682.2758 - val_loss: 1108.0476\n",
      "Epoch 653/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13682.4166 - val_loss: 1108.0920\n",
      "Epoch 654/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13684.9335 - val_loss: 1108.2184\n",
      "Epoch 655/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13690.4585 - val_loss: 1108.4742\n",
      "Epoch 656/1000\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 13697.9221 - val_loss: 1108.8914\n",
      "Epoch 00656: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, dropout=0.0, recurrent_dropout=0.0, input_shape=(None, 1)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "print('Train...')\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=1,epochs=1000)\n",
    "model.load_weights('best_weights.hdf5') # load weights from best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (RMSE): 33.28727571276187\n"
     ]
    }
   ],
   "source": [
    "# Predict and measure RMSE\n",
    "pred = model.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 833.08392334],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.08392334],\n",
       "       [ 833.08392334],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.08392334],\n",
       "       [ 833.08392334],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.08392334],\n",
       "       [ 833.08392334],\n",
       "       [ 833.08392334],\n",
       "       [ 833.08392334],\n",
       "       [ 833.08392334],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.08392334],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.0838623 ],\n",
       "       [ 833.0838623 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>total_points</th>\n",
       "      <th>previous_points</th>\n",
       "      <th>rank_change</th>\n",
       "      <th>cur_year_avg</th>\n",
       "      <th>cur_year_avg_weighted</th>\n",
       "      <th>last_year_avg</th>\n",
       "      <th>last_year_avg_weighted</th>\n",
       "      <th>two_year_ago_avg</th>\n",
       "      <th>two_year_ago_avg_weighted</th>\n",
       "      <th>three_year_ago_avg</th>\n",
       "      <th>three_year_ago_avg_weighted</th>\n",
       "      <th>country_full_class</th>\n",
       "      <th>country_abrv_class</th>\n",
       "      <th>confederation_class</th>\n",
       "      <th>rank_date_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>791.44</td>\n",
       "      <td>777</td>\n",
       "      <td>2.0</td>\n",
       "      <td>335.41</td>\n",
       "      <td>335.41</td>\n",
       "      <td>430.01</td>\n",
       "      <td>215.01</td>\n",
       "      <td>597.61</td>\n",
       "      <td>179.28</td>\n",
       "      <td>308.68</td>\n",
       "      <td>61.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>741.45</td>\n",
       "      <td>791</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>290.69</td>\n",
       "      <td>290.69</td>\n",
       "      <td>380.25</td>\n",
       "      <td>190.13</td>\n",
       "      <td>604.26</td>\n",
       "      <td>181.28</td>\n",
       "      <td>396.74</td>\n",
       "      <td>79.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>719.61</td>\n",
       "      <td>741</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>298.14</td>\n",
       "      <td>298.14</td>\n",
       "      <td>308.72</td>\n",
       "      <td>154.36</td>\n",
       "      <td>634.66</td>\n",
       "      <td>190.40</td>\n",
       "      <td>383.53</td>\n",
       "      <td>76.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>720.85</td>\n",
       "      <td>720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>720.85</td>\n",
       "      <td>721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.53</td>\n",
       "      <td>285.53</td>\n",
       "      <td>358.73</td>\n",
       "      <td>179.37</td>\n",
       "      <td>583.78</td>\n",
       "      <td>175.13</td>\n",
       "      <td>404.11</td>\n",
       "      <td>80.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  total_points  previous_points  rank_change  cur_year_avg  \\\n",
       "0    28        791.44              777          2.0        335.41   \n",
       "1    31        741.45              791         -3.0        290.69   \n",
       "2    34        719.61              741         -3.0        298.14   \n",
       "3    34        720.85              720          0.0        285.53   \n",
       "4    34        720.85              721          0.0        285.53   \n",
       "\n",
       "   cur_year_avg_weighted  last_year_avg  last_year_avg_weighted  \\\n",
       "0                 335.41         430.01                  215.01   \n",
       "1                 290.69         380.25                  190.13   \n",
       "2                 298.14         308.72                  154.36   \n",
       "3                 285.53         358.73                  179.37   \n",
       "4                 285.53         358.73                  179.37   \n",
       "\n",
       "   two_year_ago_avg  two_year_ago_avg_weighted  three_year_ago_avg  \\\n",
       "0            597.61                     179.28              308.68   \n",
       "1            604.26                     181.28              396.74   \n",
       "2            634.66                     190.40              383.53   \n",
       "3            583.78                     175.13              404.11   \n",
       "4            583.78                     175.13              404.11   \n",
       "\n",
       "   three_year_ago_avg_weighted  country_full_class  country_abrv_class  \\\n",
       "0                        61.74                   0                   0   \n",
       "1                        79.35                   0                   0   \n",
       "2                        76.71                   0                   0   \n",
       "3                        80.82                   0                   0   \n",
       "4                        80.82                   0                   0   \n",
       "\n",
       "   confederation_class  rank_date_class  \n",
       "0                    0                0  \n",
       "1                    0                1  \n",
       "2                    0                2  \n",
       "3                    0                3  \n",
       "4                    0                4  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
